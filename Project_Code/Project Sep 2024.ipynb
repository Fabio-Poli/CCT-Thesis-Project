{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbe1914-3c49-427d-9641-714756254a50",
   "metadata": {},
   "source": [
    "This project is at the intersection of `Machine Learning techniques` and `Data Analytics`, targeting on predicting the movements of `European Market Index`. Given the complex and often unpredictable nature of financial markets, traditional models have struggled to capture the full spectrum of market dynamics. This project proposes an approach by leveraging `Long Short-Term Memory (LSTM) networks`, a type of recurrent neural network that has shown exceptional promise in understanding and predicting sequences of data.\n",
    "\n",
    "LSTMs are particularly suited for this task due to their ability to remember information over extended periods, making them ideal for the analysis of time series data, which is characteristic of stock market prices. By focusing on European Market Index, this research aims to uncover patterns and signals embedded in historical data, offering insights and predictive capabilities that can inform investment strategies and financial decision-making. This notebook serves as both a documentation of the project's methodology and a comprehensive guide for replicating the predictive analysis, illustrating the integration of cutting-edge data analytics techniques within the financial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b588ce3-4497-4665-94e7-1ed5d96d4272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 14:08:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/29 14:08:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Price Prediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1977064-383c-4cc2-b439-162b3b362a57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15954dc3-851c-4a00-bb8a-ac9ebc0f0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "\n",
    "# Data Fetching and File Handling\n",
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Data Visualization Tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Preprocessing and Metrics\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Deep Learning with Keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Hyperparameter Optimization with Optuna\n",
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback\n",
    "import optuna.visualization as ov\n",
    "\n",
    "# Time Series Analysis with Statsmodels\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# PySpark for Large-Scale Data Processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, trim, when, isnan, count, mean, last, first, year\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from operator import or_\n",
    "\n",
    "# IPython Utilities for Interactive Notebooks\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Serialization and Model Persistence\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback\n",
    "import itertools\n",
    "from pmdarima import auto_arima\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e47dc39-5908-4043-8a78-17d16fc4ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae19a6-ad98-4258-9a66-19109069186b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# INDEX,Rates and Technical Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f885581-e132-4007-b4f9-32a0320ae8a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## INDEX Indicator"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32442034-3d20-4e2f-9499-d3df24701195",
   "metadata": {},
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'  # API key\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP historical data endpoint\n",
    "url = f'https://financialmodelingprep.com/api/v3/historical-price-full/{stock_symbol}?from=2005-01-02&to={datetime.now().strftime(\"%Y-%m-%d\")}&apikey={api_key}'\n",
    "\n",
    "# FMP API request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the historical data\n",
    "    if 'historical' in data:\n",
    "        historical_data = data['historical']\n",
    "\n",
    "        # Convert to a pandas DataFrame\n",
    "        index_df = pd.DataFrame(historical_data)\n",
    "\n",
    "        # Save the DataFrame to a CSV file on your desktop\n",
    "        desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')  # Path to the desktop\n",
    "        file_path = os.path.join(desktop_path, 'STOXX50E.csv')  # File Name\n",
    "        index_df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"Data saved successfully to {file_path}\")\n",
    "    else:\n",
    "        print(\"No 'historical' data found in the response.\")\n",
    "else:\n",
    "    print(f'Failed to retrieve data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd61ccf-791e-4a71-9afd-c48e59c71307",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Technical Indicators"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e326c3e-3275-47d7-b7a2-14d3fa120f12",
   "metadata": {},
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP SMA technical indicator endpoint with daily data\n",
    "sma_period = 50\n",
    "url = f'https://financialmodelingprep.com/api/v3/technical_indicator/daily/{stock_symbol}?period={sma_period}&type=sma&apikey={api_key}'\n",
    "\n",
    "# FMP API request for SMA data\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    sma_data = response.json()\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    sma_df = pd.DataFrame(sma_data)\n",
    "\n",
    "    # Ensure 'date' column is in datetime format\n",
    "    sma_df['date'] = pd.to_datetime(sma_df['date'])\n",
    "\n",
    "    # Format the 'date' column to display dates only\n",
    "    sma_df['date'] = sma_df['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Filter the DataFrame for rows where the date is after February 1, 2005\n",
    "    sma_df = sma_df[sma_df['date'] >= '2006-01-02']\n",
    "\n",
    "    # Keep only the 'date' and 'sma' columns\n",
    "    sma_df = sma_df[['date', 'sma']]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(sma_df)\n",
    "else:\n",
    "    print(f'Failed to retrieve SMA data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22c73488-f053-40ae-b573-c39567330868",
   "metadata": {},
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP EMA technical indicator endpoint with daily data\n",
    "ema_period = 50\n",
    "url = f'https://financialmodelingprep.com/api/v3/technical_indicator/daily/{stock_symbol}?period={ema_period}&type=ema&apikey={api_key}'\n",
    "\n",
    "# FMP API request for EMA data\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    ema_data = response.json()\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    ema_df = pd.DataFrame(ema_data)\n",
    "\n",
    "    # Ensure 'date' column is in datetime format\n",
    "    ema_df['date'] = pd.to_datetime(ema_df['date'])\n",
    "\n",
    "    # Format the 'date' column to display dates only\n",
    "    ema_df['date'] = ema_df['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Filter the DataFrame for rows where the date is after February 1, 2005\n",
    "    ema_df = ema_df[ema_df['date'] >= '2006-01-02']\n",
    "\n",
    "    # Keep only the 'date' and 'ema' columns\n",
    "    ema_df = ema_df[['date', 'ema']]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(ema_df)\n",
    "else:\n",
    "    print(f'Failed to retrieve EMA data: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e1bdf-a406-4511-9c74-eab67f949cf3",
   "metadata": {},
   "source": [
    "## Technical Indicator Analysis Commentary\n",
    "\n",
    "In the analysis of the `EURO STOXX 50` Index, It has been identified that the Financial Modeling Prep (FMP) API was not feasible for extracting technical indicators such as SMA, EMA, and MACD (5+ Year Historical Data). This led to the implementation of an alternative approach using the pandas library in Python.\n",
    "\n",
    "The methodology adhered to established financial analysis standards and utilized conventional computational techniques for the calculation of these indicators. The `Simple Moving Average (SMA)` and the `Exponential Moving Average (EMA)` were computed over a 50-day period. Additionally, the `Moving Average Convergence Divergence (MACD)` was calculated. Initially, the MACD's three components - the MACD line, the Signal line, and the MACD Histogram - were considered. However, the analysis was later refined to include only the MACD line to maintain focus and relevance.\n",
    "\n",
    "Utilizing pandas for this purpose offered enhanced flexibility and control in data manipulation and calculations. This approach ensured that the analysis was not constrained by the limitations of external data sources or APIs. The resulting dataset, which begins from January 2, 2006, provides a detailed view of the index's performance with accurately computed SMA, EMA, and MACD values, facilitating a thorough technical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3077d45f-4a31-4b5f-989b-5e5c0ce62b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Close          SMA          EMA         MACD\n",
      "count  4580.000000  4580.000000  4580.000000  4580.000000\n",
      "mean   3331.470597  3336.877300  3337.253795    -1.682844\n",
      "std     608.188973   605.063876   601.451103    43.794864\n",
      "min    1809.979981  2091.644585  2096.246724  -140.620963\n",
      "25%    2880.682495  2880.461552  2874.442989   -28.815001\n",
      "50%    3338.089966  3333.819001  3329.223542    -6.095822\n",
      "75%    3740.507524  3726.657101  3703.855823    19.642689\n",
      "max    4763.850100  4763.850100  4763.850100   275.182951\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file on the desktop\n",
    "desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')  # Path to the desktop\n",
    "file_path = os.path.join(desktop_path, 'STOXX50E.csv')  # File Name\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "index_df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter to only use 'date' and 'close' columns\n",
    "index_df = index_df[['date', 'close']]\n",
    "\n",
    "# Rename columns to 'Date' and 'Close'\n",
    "index_df.rename(columns={'date': 'Date', 'close': 'Close'}, inplace=True)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "index_df['Date'] = pd.to_datetime(index_df['Date'])\n",
    "\n",
    "# Calculate the 50-day Simple Moving Average (SMA)\n",
    "index_df['SMA'] = index_df['Close'].rolling(window=50, min_periods=1).mean()\n",
    "\n",
    "# Calculate the 50-day Exponential Moving Average (EMA)\n",
    "index_df['EMA'] = index_df['Close'].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "# Calculate only the MACD Line (12-day EMA - 26-day EMA)\n",
    "index_df['MACD'] = index_df['Close'].ewm(span=12, adjust=False).mean() - index_df['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "# Filter the DataFrame for dates after 2006-01-02\n",
    "index_data = index_df[index_df['Date'] >= pd.to_datetime('2006-01-02')]\n",
    "\n",
    "# Convert 'Date' from datetime to date (to remove time component)\n",
    "index_data['Date'] = index_data['Date'].dt.date\n",
    "\n",
    "# Now, use describe() on the filtered DataFrame for the numerical columns\n",
    "descriptive_stats = index_data[['Close', 'SMA', 'EMA', 'MACD']].describe()\n",
    "\n",
    "# Print the descriptive statistics\n",
    "print(descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9179752a-fdf0-4456-a3ba-baefdae201f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>MACD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4580.000000</td>\n",
       "      <td>4580.000000</td>\n",
       "      <td>4580.000000</td>\n",
       "      <td>4580.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3331.470597</td>\n",
       "      <td>3336.877300</td>\n",
       "      <td>3337.253795</td>\n",
       "      <td>-1.682844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>608.188973</td>\n",
       "      <td>605.063876</td>\n",
       "      <td>601.451103</td>\n",
       "      <td>43.794864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1809.979981</td>\n",
       "      <td>2091.644585</td>\n",
       "      <td>2096.246724</td>\n",
       "      <td>-140.620963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2880.682495</td>\n",
       "      <td>2880.461552</td>\n",
       "      <td>2874.442989</td>\n",
       "      <td>-28.815001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3338.089966</td>\n",
       "      <td>3333.819001</td>\n",
       "      <td>3329.223542</td>\n",
       "      <td>-6.095822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3740.507524</td>\n",
       "      <td>3726.657101</td>\n",
       "      <td>3703.855823</td>\n",
       "      <td>19.642689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4763.850100</td>\n",
       "      <td>4763.850100</td>\n",
       "      <td>4763.850100</td>\n",
       "      <td>275.182951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close          SMA          EMA         MACD\n",
       "count  4580.000000  4580.000000  4580.000000  4580.000000\n",
       "mean   3331.470597  3336.877300  3337.253795    -1.682844\n",
       "std     608.188973   605.063876   601.451103    43.794864\n",
       "min    1809.979981  2091.644585  2096.246724  -140.620963\n",
       "25%    2880.682495  2880.461552  2874.442989   -28.815001\n",
       "50%    3338.089966  3333.819001  3329.223542    -6.095822\n",
       "75%    3740.507524  3726.657101  3703.855823    19.642689\n",
       "max    4763.850100  4763.850100  4763.850100   275.182951"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351683e0-978a-4ab8-b419-086d287b79c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>MACD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>4763.85010</td>\n",
       "      <td>4763.850100</td>\n",
       "      <td>4763.850100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>4743.16992</td>\n",
       "      <td>4753.510010</td>\n",
       "      <td>4763.039113</td>\n",
       "      <td>-1.649701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>4709.22021</td>\n",
       "      <td>4738.746743</td>\n",
       "      <td>4760.928567</td>\n",
       "      <td>-5.631641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-13</td>\n",
       "      <td>4689.27979</td>\n",
       "      <td>4726.380005</td>\n",
       "      <td>4758.118811</td>\n",
       "      <td>-10.277905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-12</td>\n",
       "      <td>4746.35010</td>\n",
       "      <td>4730.374024</td>\n",
       "      <td>4757.657293</td>\n",
       "      <td>-9.248395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Close          SMA          EMA       MACD\n",
       "0  2024-02-16  4763.85010  4763.850100  4763.850100   0.000000\n",
       "1  2024-02-15  4743.16992  4753.510010  4763.039113  -1.649701\n",
       "2  2024-02-14  4709.22021  4738.746743  4760.928567  -5.631641\n",
       "3  2024-02-13  4689.27979  4726.380005  4758.118811 -10.277905\n",
       "4  2024-02-12  4746.35010  4730.374024  4757.657293  -9.248395"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597234e-9980-4b87-8cdd-d81bafd89a92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Interest Rate API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bab679d-b771-4e7e-9a75-f94f69e4a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"ei_mfir_m\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d475906-6051-4f34-9ff7-ec825a4f06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'ei_mfir_m.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 's_adj', 'p_adj', 'indic', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['indic'] == 'MF-3MI-RT')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq', 's_adj', 'p_adj'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from Jan, 2005, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_2006 = date_columns[date_columns >= '2005-01 ']\n",
    "structured_data = structured_data[['indic','geo'] + list(dates_from_2006)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3b6951-f3a9-4204-ac9d-85afeb01e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  geo     Period  Interest\n",
      "0  EA 2005-03-01  2.13720 \n",
      "1  EA 2005-04-01  2.13720 \n",
      "2  EA 2005-05-01  2.12560 \n",
      "3  EA 2005-06-01  2.11100 \n",
      "4  EA 2005-07-01  2.11940 \n",
      "5  EA 2005-08-01  2.13250 \n",
      "6  EA 2005-09-01  2.13910 \n",
      "7  EA 2005-10-01  2.19660 \n",
      "8  EA 2005-11-01  2.36090 \n",
      "9  EA 2005-12-01  2.47290 \n"
     ]
    }
   ],
   "source": [
    "# Ensure that the date columns include '2005-01'\n",
    "date_columns = structured_data.columns[4:]  # Starting from the 5th column\n",
    "data_for_melting = structured_data[['geo'] + list(date_columns)]\n",
    "\n",
    "# Melting the DataFrame\n",
    "melted_data = pd.melt(data_for_melting, id_vars=['geo'], var_name='Period', value_name='Interest')\n",
    "\n",
    "# Ensure 'Period' is of datetime type\n",
    "melted_data['Period'] = pd.to_datetime(melted_data['Period'])\n",
    "\n",
    "# Filter for 'geo' == 'EA'\n",
    "Interest_data_ea = melted_data[melted_data['geo'] == 'EA']\n",
    "\n",
    "# Apply forward filling\n",
    "Interest_data_ea = Interest_data_ea.sort_values('Period')\n",
    "Interest_data_ea['Interest'] = Interest_data_ea['Interest'].ffill()\n",
    "\n",
    "# Reset index for a clean DataFrame\n",
    "Interest_data_ea.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(Interest_data_ea.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0c0cc-98a9-48f6-afd2-04ee6eef3f6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Forward Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9732975b-b8b9-4542-8ec0-b0b3df73aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Period geo  Interest\n",
      "307 2006-01-02  EA  2.51170 \n",
      "308 2006-01-03  EA  2.51170 \n",
      "309 2006-01-04  EA  2.51170 \n",
      "310 2006-01-05  EA  2.51170 \n",
      "311 2006-01-06  EA  2.51170 \n",
      "314 2006-01-09  EA  2.51170 \n",
      "315 2006-01-10  EA  2.51170 \n",
      "316 2006-01-11  EA  2.51170 \n",
      "317 2006-01-12  EA  2.51170 \n",
      "318 2006-01-13  EA  2.51170 \n"
     ]
    }
   ],
   "source": [
    "# Convert 'Period' to datetime\n",
    "Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n",
    "\n",
    "# Set 'Period' as the index\n",
    "Interest_data_ea.set_index('Period', inplace=True)\n",
    "\n",
    "# Create a daily date range that covers the entire period in your data\n",
    "start_date = Interest_data_ea.index.min()\n",
    "end_date = Interest_data_ea.index.max()\n",
    "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex the DataFrame to this date range, forward filling the missing values\n",
    "daily_interest_data = Interest_data_ea.reindex(all_days).ffill()\n",
    "\n",
    "# Reset the index so 'Period' becomes a column again\n",
    "daily_interest_data.reset_index(inplace=True)\n",
    "daily_interest_data.rename(columns={'index': 'Period'}, inplace=True)\n",
    "\n",
    "# Filter for dates starting from January 2, 2006\n",
    "daily_interest_data = daily_interest_data[daily_interest_data['Period'] >= '2006-01-02']\n",
    "\n",
    "# Exclude weekends (Saturday and Sunday)\n",
    "daily_interest_data = daily_interest_data[~daily_interest_data['Period'].dt.dayofweek.isin([5, 6])]\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(daily_interest_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddb7e3-3b12-49df-9557-494a11a5e8d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### KNN Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a057cd3-9989-4651-b54c-a3276e121656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Period geo  Interest\n",
      "307 2006-01-02  EA    2.5337\n",
      "308 2006-01-03  EA    2.5337\n",
      "309 2006-01-04  EA    2.5337\n",
      "310 2006-01-05  EA    2.5337\n",
      "311 2006-01-06  EA    2.5337\n",
      "314 2006-01-09  EA    2.5337\n",
      "315 2006-01-10  EA    2.5337\n",
      "316 2006-01-11  EA    2.5337\n",
      "317 2006-01-12  EA    2.5337\n",
      "318 2006-01-13  EA    2.5337\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Period' is in the columns; if not, the index might already be the 'Period'\n",
    "if 'Period' in Interest_data_ea.columns:\n",
    "    Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n",
    "    Interest_data_ea.set_index('Period', inplace=True)\n",
    "elif not isinstance(Interest_data_ea.index, pd.DatetimeIndex):\n",
    "    # If the index is not a datetime, convert it\n",
    "    Interest_data_ea.index = pd.to_datetime(Interest_data_ea.index)\n",
    "\n",
    "# Create a daily date range\n",
    "start_date = Interest_data_ea.index.min()\n",
    "end_date = Interest_data_ea.index.max()\n",
    "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex to create a DataFrame with all days, introducing NaNs for days without data\n",
    "interest_data = Interest_data_ea.reindex(all_days)\n",
    "\n",
    "# Convert the index to a numerical value for KNN\n",
    "interest_data['Day_Ordinal'] = interest_data.index.map(datetime.toordinal)\n",
    "\n",
    "# Extract the 'Interest' column for imputation\n",
    "knn_data = interest_data[['Day_Ordinal', 'Interest']].values\n",
    "\n",
    "# Initialize KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_data_imputed = knn_imputer.fit_transform(knn_data)\n",
    "\n",
    "# Put the imputed 'Interest' data back into the DataFrame\n",
    "interest_data['Interest'] = knn_data_imputed[:, 1]\n",
    "\n",
    "# Drop the 'Day_Ordinal' column as it's no longer needed\n",
    "interest_data.drop('Day_Ordinal', axis=1, inplace=True)\n",
    "\n",
    "# Reset index to make 'Period' a column again\n",
    "interest_data.reset_index(inplace=True)\n",
    "interest_data.rename(columns={'index': 'Period'}, inplace=True)\n",
    "\n",
    "# If the 'geo' column is required, fill it with a constant value (e.g., 'EA')\n",
    "interest_data['geo'] = 'EA'\n",
    "\n",
    "# Filter for dates starting from January 2, 2006\n",
    "interest_data = interest_data[interest_data['Period'] >= '2006-01-02']\n",
    "\n",
    "# Exclude weekends (Saturday and Sunday)\n",
    "interest_data = interest_data[~interest_data['Period'].dt.dayofweek.isin([5, 6])]\n",
    "\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(interest_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f33bc0-06eb-47d2-b0bf-ad5f961dc80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Each Column:\n",
      " Period      0\n",
      "geo         0\n",
      "Interest    0\n",
      "dtype: int64\n",
      "\n",
      "NaN Values in Each Column:\n",
      " Period      0\n",
      "geo         0\n",
      "Interest    0\n",
      "dtype: int64\n",
      "\n",
      "Zero Values in Each Column:\n",
      " Period      0\n",
      "geo         0\n",
      "Interest    0\n",
      "dtype: int64\n",
      "\n",
      "N/A Values in Each Column:\n",
      " Period      0\n",
      "geo         0\n",
      "Interest    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_counts = interest_data.isnull().sum()\n",
    "print(\"Null Values in Each Column:\\n\", null_counts)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = interest_data.isna().sum()\n",
    "print(\"\\nNaN Values in Each Column:\\n\", nan_counts)\n",
    "\n",
    "# Check for zero values\n",
    "zero_counts = (interest_data == 0).sum()\n",
    "print(\"\\nZero Values in Each Column:\\n\", zero_counts)\n",
    "\n",
    "# Check for N/A values\n",
    "na_counts = (interest_data == \"N/A\").sum()\n",
    "print(\"\\nN/A Values in Each Column:\\n\", na_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75932c1a-6e81-4ce1-8f20-a644d6b56323",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bonds Yields API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c235fd6-cf50-42b0-9142-a4bfd163c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"irt_euryld_d\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3d79b6-dd27-48d4-8ee7-d78e53cb60f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'irt_euryld_d.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 'yld_curv', 'maturity', 'bonds', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['yld_curv'] == 'SPOT_RT') & \n",
    "    (structured_data['bonds'] == 'CGB_EA')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq', 'geo'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from January 2, 2006, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2006-01-02 ']\n",
    "structured_data = structured_data[['yld_curv', 'maturity', 'bonds'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eca047b-9076-46cf-a25e-e8496b7bc7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Period maturity     Rate\n",
      "0  2006-01-02       M10  2.63864\n",
      "1  2006-01-02       M11   2.6671\n",
      "2  2006-01-02        M3  2.34702\n",
      "3  2006-01-02        M4  2.40081\n",
      "4  2006-01-02        M5  2.44996\n"
     ]
    }
   ],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[3:]  # Assuming date columns start from the 4th column\n",
    "data_for_melting = structured_data[['maturity'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "bonds_data = pd.melt(data_for_melting, id_vars=['maturity'], var_name='Period', value_name='Rate')\n",
    "\n",
    "# Specify the desired maturities (M3 to M11 and Y1 to Y30)\n",
    "desired_maturities = [f'M{i}' for i in range(0, 13)] + [f'Y{i}' for i in range(1, 31)]\n",
    "\n",
    "# Filter the DataFrame to include only the specified maturities\n",
    "bonds_data = bonds_data[bonds_data['maturity'].isin(desired_maturities)]\n",
    "\n",
    "# Rearrange the columns\n",
    "bonds_data = bonds_data[['Period', 'maturity', 'Rate']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(bonds_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a599051-9819-4582-aa8b-aef4232cf69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Each Column:\n",
      " Period      0\n",
      "maturity    0\n",
      "Rate        0\n",
      "dtype: int64\n",
      "\n",
      "NaN Values in Each Column:\n",
      " Period      0\n",
      "maturity    0\n",
      "Rate        0\n",
      "dtype: int64\n",
      "\n",
      "Zero Values in Each Column:\n",
      " Period      0\n",
      "maturity    0\n",
      "Rate        0\n",
      "dtype: int64\n",
      "\n",
      "N/A Values in Each Column:\n",
      " Period      0\n",
      "maturity    0\n",
      "Rate        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_counts = bonds_data.isnull().sum()\n",
    "print(\"Null Values in Each Column:\\n\", null_counts)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = bonds_data.isna().sum()\n",
    "print(\"\\nNaN Values in Each Column:\\n\", nan_counts)\n",
    "\n",
    "# Check for zero values\n",
    "zero_counts = (bonds_data == 0).sum()\n",
    "print(\"\\nZero Values in Each Column:\\n\", zero_counts)\n",
    "\n",
    "# Check for N/A values\n",
    "na_counts = (bonds_data == \"N/A\").sum()\n",
    "print(\"\\nN/A Values in Each Column:\\n\", na_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2005f-ad08-4054-a554-9c035972bd62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inflation Rate API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155c37e4-15a3-4a40-83f4-912786f10e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"prc_hicp_manr\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a534f6-4c7e-4161-acc1-e454d0264550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'prc_hicp_manr.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 'unit', 'coicop', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['unit'] == 'RCH_A') & \n",
    "    (structured_data['coicop'] == 'CP00')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from January 1, 2005, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2005-01']\n",
    "structured_data = structured_data[['unit', 'coicop', 'geo'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87fc8e93-4664-47e4-9e3c-e3ced50311eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  geo     Period Inflation\n",
      "0  EA 2005-01-01      1.9 \n",
      "1  EA 2005-02-01      2.1 \n",
      "2  EA 2005-03-01      2.2 \n",
      "3  EA 2005-04-01      2.1 \n",
      "4  EA 2005-05-01      2.0 \n",
      "5  EA 2005-06-01      2.0 \n",
      "6  EA 2005-07-01      2.1 \n",
      "7  EA 2005-08-01      2.2 \n",
      "8  EA 2005-09-01      2.6 \n",
      "9  EA 2005-10-01      2.5 \n"
     ]
    }
   ],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[3:]  # Include all date columns starting from the 4th column\n",
    "data_for_melting = structured_data[['geo'] + list(date_columns)]\n",
    "\n",
    "# Melting the DataFrame\n",
    "melted_data = pd.melt(data_for_melting, id_vars=['geo'], var_name='Period', value_name='Inflation')\n",
    "\n",
    "# Filter for 'geo' == 'EA' and convert 'Period' to datetime format\n",
    "inflation_data_ea = melted_data[melted_data['geo'] == 'EA']\n",
    "inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "\n",
    "# Ensure the DataFrame is sorted by period and forward fill missing 'Inflation' values\n",
    "inflation_data_ea = inflation_data_ea.sort_values(by='Period')\n",
    "inflation_data_ea['Inflation'] = inflation_data_ea.groupby('geo')['Inflation'].ffill()\n",
    "\n",
    "# Reset the index and display the DataFrame\n",
    "inflation_data_ea = inflation_data_ea.reset_index(drop=True)\n",
    "print(inflation_data_ea.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78081083-ef0a-4148-a9aa-8276cfcf38ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Forward Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af5f1ae9-ac11-4cec-bbd9-bca717be51c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Period geo Inflation\n",
      "366 2006-01-02  EA      2.4 \n",
      "367 2006-01-03  EA      2.4 \n",
      "368 2006-01-04  EA      2.4 \n",
      "369 2006-01-05  EA      2.4 \n",
      "370 2006-01-06  EA      2.4 \n",
      "373 2006-01-09  EA      2.4 \n",
      "374 2006-01-10  EA      2.4 \n",
      "375 2006-01-11  EA      2.4 \n",
      "376 2006-01-12  EA      2.4 \n",
      "377 2006-01-13  EA      2.4 \n"
     ]
    }
   ],
   "source": [
    "# Convert 'Period' to datetime\n",
    "inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "\n",
    "# Set 'Period' as the DataFrame index\n",
    "inflation_data_ea.set_index('Period', inplace=True)\n",
    "\n",
    "# Resample to daily data using forward fill\n",
    "daily_data_ffill = inflation_data_ea.resample('D').ffill()\n",
    "\n",
    "# Reset index to move 'Period' back to a column\n",
    "daily_data_ffill.reset_index(inplace=True)\n",
    "\n",
    "# Filter for dates starting from January 2, 2006\n",
    "daily_data_ffill = daily_data_ffill[daily_data_ffill['Period'] >= '2006-01-02']\n",
    "\n",
    "# Exclude weekends (Saturday and Sunday)\n",
    "daily_data_ffill = daily_data_ffill[~daily_data_ffill['Period'].dt.dayofweek.isin([5, 6])]\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(daily_data_ffill.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74016a4a-760e-4e3a-8879-4d982de87239",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### KNN Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0a6b469-d155-4ffb-aa15-039b91c124db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Period geo  Inflation\n",
      "366 2006-01-02  EA       2.32\n",
      "367 2006-01-03  EA       2.32\n",
      "368 2006-01-04  EA       2.32\n",
      "369 2006-01-05  EA       2.32\n",
      "370 2006-01-06  EA       2.32\n",
      "373 2006-01-09  EA       2.32\n",
      "374 2006-01-10  EA       2.32\n",
      "375 2006-01-11  EA       2.32\n",
      "376 2006-01-12  EA       2.32\n",
      "377 2006-01-13  EA       2.32\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Period' is in the columns; if not, the index should already be the 'Period'\n",
    "if 'Period' in inflation_data_ea.columns:\n",
    "    inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "    inflation_data_ea.set_index('Period', inplace=True)\n",
    "elif not isinstance(inflation_data_ea.index, pd.DatetimeIndex):\n",
    "    # If the index is not a datetime, convert it\n",
    "    inflation_data_ea.index = pd.to_datetime(inflation_data_ea.index)\n",
    "\n",
    "# Convert non-numeric values to NaN in the 'Inflation' column\n",
    "inflation_data_ea['Inflation'] = pd.to_numeric(inflation_data_ea['Inflation'], errors='coerce')\n",
    "\n",
    "# Create a daily date range that covers the entire period of your data\n",
    "date_range = pd.date_range(start=inflation_data_ea.index.min(), end=inflation_data_ea.index.max(), freq='D')\n",
    "\n",
    "# Create a new DataFrame for KNN imputation\n",
    "inflation_data = pd.DataFrame(date_range, columns=['Period'])\n",
    "\n",
    "# Merge with your existing data - this will introduce NaNs for days without data\n",
    "inflation_data = inflation_data.merge(inflation_data_ea, how='left', left_on='Period', right_index=True)\n",
    "\n",
    "# Convert 'Period' to a numerical value (e.g., days since start)\n",
    "min_date = inflation_data['Period'].min()\n",
    "inflation_data['Day_Ordinal'] = (inflation_data['Period'] - min_date).dt.days\n",
    "\n",
    "# Prepare data for KNN imputation\n",
    "knn_data = inflation_data[['Day_Ordinal', 'Inflation']]\n",
    "\n",
    "# Initialize KNN Imputer - 5 neighbors / Week\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_data_imputed = knn_imputer.fit_transform(knn_data)\n",
    "\n",
    "# Add imputed data back to DataFrame\n",
    "inflation_data['Inflation'] = knn_data_imputed[:, 1]\n",
    "\n",
    "# Fill 'geo' column with 'EA'\n",
    "inflation_data['geo'] = 'EA'\n",
    "\n",
    "# Convert 'Day_Ordinal' back to datetime\n",
    "inflation_data['Period'] = inflation_data['Day_Ordinal'].apply(lambda x: min_date + timedelta(days=x))\n",
    "\n",
    "# Drop the 'Day_Ordinal' column\n",
    "inflation_data.drop(columns='Day_Ordinal', inplace=True)\n",
    "\n",
    "# Filter for dates starting from January 2, 2006\n",
    "inflation_data = inflation_data[inflation_data['Period'] >= '2006-01-02']\n",
    "\n",
    "# Exclude weekends (Saturday and Sunday)\n",
    "inflation_data = inflation_data[~inflation_data['Period'].dt.dayofweek.isin([5, 6])]\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(inflation_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17d7b673-02a2-449e-856a-3af4415f1f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Each Column:\n",
      " Period       0\n",
      "geo          0\n",
      "Inflation    0\n",
      "dtype: int64\n",
      "\n",
      "NaN Values in Each Column:\n",
      " Period       0\n",
      "geo          0\n",
      "Inflation    0\n",
      "dtype: int64\n",
      "\n",
      "Zero Values in Each Column:\n",
      " Period       0\n",
      "geo          0\n",
      "Inflation    3\n",
      "dtype: int64\n",
      "\n",
      "N/A Values in Each Column:\n",
      " Period       0\n",
      "geo          0\n",
      "Inflation    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_counts = inflation_data.isnull().sum()\n",
    "print(\"Null Values in Each Column:\\n\", null_counts)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = inflation_data.isna().sum()\n",
    "print(\"\\nNaN Values in Each Column:\\n\", nan_counts)\n",
    "\n",
    "# Check for zero values\n",
    "zero_counts = (inflation_data == 0).sum()\n",
    "print(\"\\nZero Values in Each Column:\\n\", zero_counts)\n",
    "\n",
    "# Check for N/A values\n",
    "na_counts = (inflation_data == \"N/A\").sum()\n",
    "print(\"\\nN/A Values in Each Column:\\n\", na_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf54c7b-3244-4834-82c5-c8b858201b75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SparkSQL: Tables Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40750bc0-b9d3-4a28-abc9-bd5937c6ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 14:08:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/08/29 14:08:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+------------------+------------------+------------+------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|      Date|     Close|               MACD|               SMA|               EMA|InterestRate|     InflationRate|     M3|     M6|     M9|     Y1|     Y3|     Y5|    Y10|    Y20|    Y30|\n",
      "+----------+----------+-------------------+------------------+------------------+------------+------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|2024-02-05|4655.27002| -21.39123992203531|       4710.351027|4745.2460523204845|     3.91808|              2.62|3.88726| 3.6421|3.43694|3.26609|2.66683|2.66288|3.01592| 3.3946|3.46675|\n",
      "|2024-02-16| 4763.8501|                0.0|         4763.8501|         4763.8501|      3.8942|2.5599999999999996|3.89213|3.69046|3.51988|3.37617|2.84118|2.80438|3.07186|3.39073|3.44119|\n",
      "|2024-02-09|4715.87012|-10.767856403392216| 4727.956706666667|   4756.0185806055|     3.91808|              2.62|3.87459|3.65706|3.47514|3.32373|2.79249|2.78712|3.09872|3.43711|3.48968|\n",
      "|2024-02-15|4743.16992| -1.649700968660909|        4753.51001|  4763.03911254902|      3.8942|              2.62|3.88657|3.66274|3.47516|3.31861|2.75732| 2.7309|3.00724|3.32039|3.37501|\n",
      "|2024-02-08|4710.77979|-12.241672931538233| 4725.502861428571| 4754.244510385676|     3.91808|              2.62|3.87676|3.64342|3.44876|3.28718| 2.7277|2.72893| 3.0638|3.41999|3.47984|\n",
      "|2024-02-06|4690.87012| -17.45596868051689| 4716.471138888888| 4748.918543435606|     3.91808|              2.62|3.89531|3.64277|3.43304|3.25976|2.67184|2.68108|3.03313|3.39753|3.46309|\n",
      "|2024-02-12| 4746.3501|  -9.24839535978208|       4730.374024| 4757.657293283275|     3.91808|              2.62|3.87798|3.65322|3.46573|3.31005|2.76839|2.76369|3.07432|3.41229|3.46938|\n",
      "|2024-02-07| 4678.8501|-15.803964024171364|4719.6712662499995|  4751.28786684114|     3.91808|              2.62|3.87939|3.63449|3.42999|3.26007|2.66976| 2.6698|3.01994|3.39061|3.45751|\n",
      "|2024-02-13|4689.27979|-10.277904567977203|       4726.380005|  4758.11881137647|     3.91808|              2.62|3.89022|3.67241|3.49063|3.33961|2.81045|2.79897|3.08602|3.40336|3.45503|\n",
      "|2024-02-02| 4654.5498| -24.28810827454072| 4705.278188181818| 4741.689336543211|     3.91808|              2.62|3.89117|3.63264|3.41623|3.23594|2.59992|2.58696|2.93277|3.30465|3.38329|\n",
      "+----------+----------+-------------------+------------------+------------------+------------+------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Market Trends Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to format date columns in Pandas DataFrame\n",
    "def format_date_columns(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column]).dt.date\n",
    "    return df\n",
    "\n",
    "# Format date columns\n",
    "index_data = format_date_columns(index_data, 'Date')\n",
    "interest_data = format_date_columns(interest_data, 'Period')\n",
    "bonds_data = format_date_columns(bonds_data, 'Period')\n",
    "inflation_data = format_date_columns(inflation_data, 'Period')\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "index_df_spark = spark.createDataFrame(index_data)\n",
    "interest_df_spark = spark.createDataFrame(interest_data)\n",
    "bonds_df_spark = spark.createDataFrame(bonds_data)\n",
    "inflation_df_spark = spark.createDataFrame(inflation_data)\n",
    "\n",
    "# Trim and convert date columns to DateType\n",
    "index_df_spark = index_df_spark.withColumn(\"Date\", to_date(trim(col(\"Date\")), \"yyyy-MM-dd\"))\n",
    "interest_df_spark = interest_df_spark.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "bonds_df_spark = bonds_df_spark.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "inflation_df_spark = inflation_df_spark.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Create Temporary Views\n",
    "index_df_spark.createOrReplaceTempView(\"indexView\")\n",
    "interest_df_spark.createOrReplaceTempView(\"interestView\")\n",
    "bonds_df_spark.createOrReplaceTempView(\"bondsView\")\n",
    "inflation_df_spark.createOrReplaceTempView(\"inflationView\")\n",
    "\n",
    "# Pivot and process bonds data to have maturity rates as separate columns\n",
    "bonds_df_pivoted = bonds_df_spark.groupBy(\"Period\").pivot(\"maturity\").agg(first(\"Rate\"))\n",
    "bonds_df_pivoted.createOrReplaceTempView(\"bondsPivotedView\")\n",
    "\n",
    "# SQL Query for Joining\n",
    "join_query = \"\"\"\n",
    "SELECT i.Date, i.Close, i.MACD, i.SMA, i.EMA,\n",
    "       int.Interest as InterestRate, \n",
    "       inf.Inflation as InflationRate,\n",
    "       b.M3, b.M6, b.M9, b.Y1, b.Y3, b.Y5, b.Y10, b.Y20, b.Y30\n",
    "FROM indexView i\n",
    "LEFT JOIN interestView int ON i.Date = int.Period\n",
    "LEFT JOIN bondsPivotedView b ON i.Date = b.Period\n",
    "LEFT JOIN inflationView inf ON i.Date = inf.Period\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and show the Result\n",
    "dataset = spark.sql(join_query)\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19e74e-3c2f-493d-9098-de40eb16e475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Date': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'Close': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'MACD': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'SMA': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'EMA': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'InterestRate': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'InflationRate': Zeros = N/A, Nulls or NaNs = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'M3': Zeros = N/A, Nulls or NaNs = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'M6': Zeros = N/A, Nulls or NaNs = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'M9': Zeros = N/A, Nulls or NaNs = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Y1': Zeros = N/A, Nulls or NaNs = 15\n"
     ]
    }
   ],
   "source": [
    "# DataFrame: joined_df\n",
    "numeric_column_types = ['int', 'double', 'float', 'long']\n",
    "\n",
    "# Counting zeros and nulls for each column\n",
    "for column in dataset.columns:\n",
    "    # Check if the column is numeric\n",
    "    if str(dataset.schema[column].dataType) in numeric_column_types:\n",
    "        zero_count = dataset.filter(col(column) == 0).count()\n",
    "        null_or_nan_count = joined_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    else:\n",
    "        zero_count = 'N/A' # Not applicable for non-numeric columns\n",
    "        null_or_nan_count = dataset.filter(col(column).isNull()).count() # Check only for nulls in non-numeric columns\n",
    "\n",
    "    print(f\"Column '{column}': Zeros = {zero_count}, Nulls or NaNs = {null_or_nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a2b69-23f8-4c3b-a9c4-92b950cea9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame: dataset\n",
    "numeric_column_types = ['int', 'double', 'float', 'long']\n",
    "\n",
    "# Initialize an empty dictionary to store columns with non-zero null, NaN, or N/A counts\n",
    "columns_with_missing_values = {}\n",
    "\n",
    "# Counting zeros and nulls for each column\n",
    "for column in dataset.columns:\n",
    "    # Check if the column is numeric\n",
    "    if str(dataset.schema[column].dataType) in numeric_column_types:\n",
    "        zero_count = dataset.filter(col(column) == 0).count()\n",
    "        null_or_nan_count = dataset.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    else:\n",
    "        zero_count = 'N/A'  # Not applicable for non-numeric columns\n",
    "        null_or_nan_count = dataset.filter(col(column).isNull()).count()  # Check only for nulls in non-numeric columns\n",
    "\n",
    "    # Add to dictionary if there are non-zero nulls, NaNs, or zeros for numeric columns\n",
    "    if zero_count != 'N/A':\n",
    "        if null_or_nan_count > 0 or zero_count > 0:\n",
    "            columns_with_missing_values[column] = {'Zeros': zero_count, 'Nulls_or_NaNs': null_or_nan_count}\n",
    "    else:\n",
    "        if null_or_nan_count > 0:\n",
    "            columns_with_missing_values[column] = {'Zeros': zero_count, 'Nulls_or_NaNs': null_or_nan_count}\n",
    "\n",
    "# Display columns with missing values\n",
    "for column, counts in columns_with_missing_values.items():\n",
    "    print(f\"Column '{column}': Zeros = {counts['Zeros']}, Nulls or NaNs = {counts['Nulls_or_NaNs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb69f4-1457-46e5-b0bf-76ae0a8041de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "\n",
    "# Function to create a column condition for checking if a column is null or NaN\n",
    "def is_missing(column_name):\n",
    "    return col(column_name).isNull() | isnan(col(column_name))\n",
    "\n",
    "# Create a combined condition for all bond rate columns\n",
    "bond_rate_conditions = is_missing('M3') | is_missing('M6') | is_missing('M9') | \\\n",
    "                       is_missing('Y1') | is_missing('Y3') | is_missing('Y5') | \\\n",
    "                       is_missing('Y10') | is_missing('Y20') | is_missing('Y30')\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset.count()\n",
    "\n",
    "# Count rows where any one of the specific columns is missing\n",
    "missing_columns_count = dataset.filter(\n",
    "    is_missing(\"InterestRate\") |\n",
    "    bond_rate_conditions |\n",
    "    is_missing(\"InflationRate\") &\n",
    "    col(\"Close\").isNotNull()\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_columns = (missing_columns_count / total_rows) * 100\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Close' but missing rates: {missing_columns_count}\")\n",
    "print(f\"Percentage of Rows with missing any rate: {percentage_missing_columns:.2f}%\")\n",
    "\n",
    "# Filter the DataFrame to get rows with missing values in specific columns\n",
    "missing_values_table = dataset.filter(\n",
    "    is_missing(\"InterestRate\") |\n",
    "    bond_rate_conditions |\n",
    "    is_missing(\"InflationRate\") &\n",
    "    col(\"Close\").isNotNull()\n",
    ")\n",
    "\n",
    "# Show the rows with missing values\n",
    "missing_values_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97a36b-25c6-41ba-8fe3-d39c9f0d50a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Window Approuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8113aa-61d8-449c-876d-ebb9ae4f90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Date' column can be used for partitioning\n",
    "window_spec_forward = Window.partitionBy(year(\"Date\")).orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window_spec_backward = Window.partitionBy(year(\"Date\")).orderBy(\"Date\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "# List of all columns to be filled\n",
    "columns_to_fill = ['InterestRate', 'InflationRate'] + ['M3', 'M6', 'M9', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']\n",
    "\n",
    "# Apply forward fill followed by backward fill for each column\n",
    "for col_name in columns_to_fill:\n",
    "    dataset_ff = dataset.withColumn(col_name, last(col_name, ignorenulls=True).over(window_spec_forward))\n",
    "    dataset_ff = dataset_ff.withColumn(col_name, last(col_name, ignorenulls=True).over(window_spec_backward))\n",
    "\n",
    "# Show the result\n",
    "dataset_ff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acd7aa-ea84-498b-be25-10ffdcdfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined condition for all bond rate columns\n",
    "bond_rate_conditions = (\n",
    "    col(\"M3\").isNull() | isnan(col(\"M3\")) |\n",
    "    col(\"M6\").isNull() | isnan(col(\"M6\")) |\n",
    "    col(\"M9\").isNull() | isnan(col(\"M9\")) |\n",
    "    col(\"Y1\").isNull() | isnan(col(\"Y1\")) |\n",
    "    col(\"Y3\").isNull() | isnan(col(\"Y3\")) |\n",
    "    col(\"Y5\").isNull() | isnan(col(\"Y5\")) |\n",
    "    col(\"Y10\").isNull() | isnan(col(\"Y10\")) |\n",
    "    col(\"Y20\").isNull() | isnan(col(\"Y20\")) |\n",
    "    col(\"Y30\").isNull() | isnan(col(\"Y30\"))\n",
    ")\n",
    "\n",
    "# Count rows where 'Close' is available but any one of 'InterestRate', bond rate columns, or 'InflationRate' is missing (Null or NaN)\n",
    "missing_any_column_count = dataset_ff.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        bond_rate_conditions |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing any one of 'InterestRate', 'BondRate', or 'InflationRate': {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing any one rate: {percentage_missing_any_column:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0720-ee51-46c6-8f65-26a5d98f7a7f",
   "metadata": {},
   "source": [
    "Here's why the Window function is used:\n",
    "\n",
    "    Row-wise Calculations: PySpark's Window function allows you to specify a range of rows (a window) relative to the current row and perform calculations across these rows.\n",
    "\n",
    "    Emulating ffill: By ordering the rows in the window based on the 'date' column and then using the last function (which returns the last non-null value in the window), we effectively emulate the forward fill behavior. It picks the last non-null value encountered in the ordered window and applies it to the current row if the current value is null.\n",
    "\n",
    "    Scalability and Performance: PySpark is designed for big data processing. While Pandas' ffill is straightforward and efficient for in-memory data on a single machine, PySpark's window functions are designed to work efficiently on distributed datasets across multiple nodes in a cluster.\n",
    "\n",
    "    Lack of Direct ffill Equivalent: PySpark DataFrame API does not have a direct method like ffill in Pandas. Therefore, the combination of Window and last functions is a common approach to achieve similar functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca8e98-4285-4e30-9fb5-e02a0976a185",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### MEAN to fill the Missing Day Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fc724-1340-4dda-b937-3b8ad88963a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numeric columns for interest rate, inflation rate, and bond rates\n",
    "numeric_cols = [\"InterestRate\", \"InflationRate\"] + ['M3', 'M6', 'M9', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']\n",
    "\n",
    "# Calculate the mean of non-null values for each column\n",
    "mean_values = dataset_ff.select([mean(c).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "\n",
    "# Fill null values with the mean for each column\n",
    "for col in numeric_cols:\n",
    "    dataset_ff = dataset_ff.na.fill({col: mean_values[col]})\n",
    "\n",
    "# Show the result\n",
    "dataset_ff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620395c1-be42-4fef-b572-d878a14eb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the bond rate columns\n",
    "bond_rate_cols = ['M3', 'M6', 'M9', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']\n",
    "\n",
    "# Create a combined condition to check for missing values in bond rate columns\n",
    "bond_rate_missing_condition = [col(c).isNull() | isnan(col(c)) for c in bond_rate_cols]\n",
    "\n",
    "# Combine all conditions using a logical OR\n",
    "combined_bond_rate_condition = reduce(lambda x, y: x | y, bond_rate_missing_condition)\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset_ff.count()\n",
    "\n",
    "# Count rows where 'Close' is available but any one of the rate columns is missing (Null or NaN)\n",
    "missing_any_column_count = dataset_ff.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\")) |\n",
    "        combined_bond_rate_condition\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Close' but missing any rate information: {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing rate information: {percentage_missing_any_column:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161b8d8-262e-4e01-8df9-2895f2c064a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## KNN Approuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6f2cf-270c-424a-befa-0e0645596058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "dataset_pd = dataset.toPandas()\n",
    "\n",
    "# Define the list of numeric columns including interest rate, inflation rate, and bond rate columns\n",
    "numeric_cols = [\"InterestRate\", \"InflationRate\", \"M3\", \"M6\", \"M9\", \"Y1\", \"Y3\", \"Y5\", \"Y10\", \"Y20\", \"Y30\"]\n",
    "\n",
    "# Replace non-numeric values to NaN\n",
    "dataset_pd[numeric_cols] = dataset_pd[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "dataset_pd[numeric_cols] = knn_imputer.fit_transform(dataset_pd[numeric_cols])\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "dataset_knn = spark.createDataFrame(dataset_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d842e-51a4-463c-a05e-e766461d9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame in ascending order by the 'date' column\n",
    "dataset_knn = dataset_knn.orderBy(\"Date\")\n",
    "\n",
    "# Show the result\n",
    "dataset_knn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e162a-beaf-458a-9cbd-0156b604b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all Bond Rate columns\n",
    "bond_rate_columns = [\"M3\", \"M6\", \"M9\", \"Y1\", \"Y3\", \"Y5\", \"Y10\", \"Y20\", \"Y30\"]\n",
    "\n",
    "# Create a condition to check for missing values in any bond rate column\n",
    "bond_rate_missing_condition = reduce(or_, [col(name).isNull() | isnan(col(name)) for name in bond_rate_columns])\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset_knn.count()\n",
    "\n",
    "# Count rows where 'Close' is available but any one of the rate columns is missing (Null or NaN)\n",
    "missing_any_column_count = dataset_knn.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\")) |\n",
    "        bond_rate_missing_condition\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Close' but missing any rate information: {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing rate information: {percentage_missing_any_column:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa1d25-3be4-460c-9807-b0afbb20e46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dd3e57c-f326-4b0b-a9d4-92304967bbb6",
   "metadata": {},
   "source": [
    "# ARIMA & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbda2c-b9d0-4c01-9245-6a13b733457b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1- Load and Structure the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0d435-d002-40e8-b7b9-324ae53a6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset_knn is a Spark DataFrame, convert it to a Pandas DataFrame\n",
    "dataset_knn_pandas = dataset_knn.toPandas()\n",
    "\n",
    "# Sort the DataFrame in ascending order by the 'Date' column\n",
    "dataset_knn_pandas = dataset_knn_pandas.sort_values(by='Date')\n",
    "\n",
    "# Ensure the 'Date' column is datetime\n",
    "dataset_knn_pandas['Date'] = pd.to_datetime(dataset_knn_pandas['Date'])\n",
    "\n",
    "# Ensure 'Close' is a float and drop NaN values\n",
    "dataset_knn_pandas['Close'] = dataset_knn_pandas['Close'].astype(float)\n",
    "dataset_knn_pandas.dropna(subset=['Close'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efb5a9-066b-452f-8694-76eaa562089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to date format (if not already done)\n",
    "dataset_knn = dataset_knn.withColumn('Date', to_date(col('Date')))\n",
    "\n",
    "# Define the start and end dates for the date range\n",
    "start_date = '2007-01-02'\n",
    "end_date = '2024-02-13'\n",
    "\n",
    "# Filter the dataset to include only rows within the specified date range\n",
    "dataset_knn = dataset_knn.filter((col('Date') >= start_date) & (col('Date') <= end_date))\n",
    "\n",
    "# Optionally, drop rows with any NaN values\n",
    "dataset_knn = dataset_knn.na.drop()\n",
    "\n",
    "# Show the max and min dates in the capped dataset to verify\n",
    "min_date = dataset_knn.agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "max_date = dataset_knn.agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "print(f\"Data capped from: {min_date} to {max_date}\")\n",
    "\n",
    "# Display the first 10 rows to verify the dataset\n",
    "dataset_knn.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80db69f-857d-44e1-a75a-9bdf990c00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_knn_pandas.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e824732-2e6b-483c-8ec7-c947e986599d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset_knn_pandas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c27f1-7608-4394-b0e9-9782ff16ae40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2- Stationarize the Data (For ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f5432-16bf-43d4-ae2e-f85ae92b3333",
   "metadata": {},
   "source": [
    "### 2.1- Analyze if the Dataset is Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae611e7d-031b-4561-ac79-05e3cd1427e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "dataset_knn_pandas = dataset_knn.toPandas()\n",
    "\n",
    "# Ensure 'Close' is a float and drop NaN values\n",
    "dataset_knn_pandas['Close'] = dataset_knn_pandas['Close'].astype(float)\n",
    "dataset_knn_pandas.dropna(subset=['Close'], inplace=True)\n",
    "\n",
    "# Function to perform Augmented Dickey-Fuller test\n",
    "def adf_test(series):\n",
    "    result = adfuller(series.dropna())\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "    return result[1]  # Return the p-value\n",
    "\n",
    "# Perform ADF test on the Close price\n",
    "p_value = adf_test(dataset_knn_pandas['Close'])\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(\"The data is non-stationary.\")\n",
    "else:\n",
    "    print(\"The data is stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e82833-60b3-458d-9dd4-d41492b7fbe0",
   "metadata": {},
   "source": [
    "### 2.2 Turn the Data Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41ed87-205c-4827-a49e-fec9fe1af11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing to make data stationary if necessary\n",
    "differencing_order = 0\n",
    "while p_value > 0.05:\n",
    "    differencing_order += 1\n",
    "    dataset_knn_pandas['Close_diff'] = dataset_knn_pandas['Close'].diff(differencing_order).dropna()\n",
    "    p_value = adf_test(dataset_knn_pandas['Close_diff'])\n",
    "\n",
    "print(f'Data is stationary after {differencing_order} differencing(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633f62b-e66b-4b65-a948-0cec8177b977",
   "metadata": {},
   "source": [
    "### 2.2 Recheck ADF after Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed625b-d1b8-40c9-b92d-64cf0d9d4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck ADF after differencing\n",
    "p_value_diff = adf_test(dataset_knn_pandas['Close_diff'])\n",
    "if p_value_diff > 0.05:\n",
    "    print(\"The differenced data is still non-stationary.\")\n",
    "else:\n",
    "    print(\"The differenced data is now stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6a2ab-6295-4b27-875b-410a0f791d53",
   "metadata": {},
   "source": [
    "### 2.3 Plot Non-Stationary and Stationary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf0afa-1453-4e6c-b0aa-69a3fb4d1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot non-stationary data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dataset_knn_pandas['Close'])\n",
    "plt.title('Non-Stationary Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "\n",
    "# Plot stationary data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dataset_knn_pandas['Close_diff'].dropna())\n",
    "plt.title('Stationary Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Differenced Close Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6b01f-f241-4db3-a161-b2bbfaf804fe",
   "metadata": {},
   "source": [
    "### 2.4 Partial Autocorrelation (PACF) and Autocorrelation (ACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe1bf5-bdd5-4707-ac03-ac32d6af4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Plot ACF and PACF\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(dataset_knn_pandas['Close_diff'].dropna(), lags=40, ax=plt.gca())\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(dataset_knn_pandas['Close_diff'].dropna(), lags=40, ax=plt.gca())\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d700a1-1f51-45c4-9ab0-bba968d23cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_knn_pandas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b34ad-a263-4ad7-96bd-2c17d1ef1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' is set as index\n",
    "if 'Date' in dataset_knn_pandas.columns:\n",
    "    dataset_knn_pandas['Date'] = pd.to_datetime(dataset_knn_pandas['Date'])\n",
    "    dataset_knn_pandas.set_index('Date', inplace=True)\n",
    "else:\n",
    "    dataset_knn_pandas.index = pd.to_datetime(dataset_knn_pandas.index)\n",
    "\n",
    "# Resample the data to the yearly frequency to observe seasonality\n",
    "def plot_seasonality(data, value_col, freq='Y'):\n",
    "    data_resampled = data[value_col].resample(freq).mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data_resampled, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Seasonality of {value_col} - {freq} Frequency')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(value_col)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_seasonality(dataset_knn_pandas, 'Close_diff', freq='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82665a25-638c-403a-85fe-011992a0ba86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "plt.figure(figsize=(12,8))\n",
    "lag_plot(dataset_knn_pandas['Close'], lag=1)\n",
    "plt.title('Close Autocorrelation plot')\n",
    "plt.grid(True)\n",
    "plt.legend();\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "values = DataFrame(dataset_knn_pandas['Close_diff'].values)\n",
    "dataframe = concat([values.shift(120), values], axis=1)\n",
    "dataframe.columns = ['t-1', 't+1']\n",
    "result = dataframe.corr()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2bce6d-1102-4c24-a6cf-e9aba9565165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a CSV file\n",
    "dataset_knn_pandas.to_csv('dataset_knn_pandas.csv')\n",
    "\n",
    "# Save the dataset to a pickle file\n",
    "dataset_knn_pandas.to_pickle('dataset_knn_pandas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fab1c0-13b6-4cbe-b803-d0ff4852ab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a download link for the CSV file\n",
    "FileLink('dataset_knn_pandas.csv')\n",
    "\n",
    "# Create a download link for the Pickle file\n",
    "FileLink('dataset_knn_pandas.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510abfe5-5043-49a5-92dd-a602af629cc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3- Build and Evaluate ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731376e7-c8bf-4ff6-9a66-8996a0b5946b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6070a1-986e-40bf-8156-56bff7c2c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "\n",
    "series = dataset_knn_pandas['Close']\n",
    "\n",
    "# Define the p, d, and q parameters to take values between 0 and 2\n",
    "p = d = q = range(0, 3)\n",
    "\n",
    "# Generate all different combinations of p, d, and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Iterate over all combinations and fit ARIMA models\n",
    "model_results = []\n",
    "for param in pdq:\n",
    "    try:\n",
    "        model = ARIMA(series, order=param)\n",
    "        results = model.fit()\n",
    "        model_results.append({\n",
    "            'Order': param,\n",
    "            'AIC': results.aic,\n",
    "            'BIC': results.bic\n",
    "        })\n",
    "        print(f'ARIMA{param} - AIC:{results.aic} - BIC:{results.bic}')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Sort by AIC and BIC\n",
    "sorted_results_aic = results_df.sort_values(by='AIC')\n",
    "sorted_results_bic = results_df.sort_values(by='BIC')\n",
    "\n",
    "# Display the top models based on AIC and BIC\n",
    "print(\"\\nTop models based on AIC:\")\n",
    "print(sorted_results_aic.head())\n",
    "\n",
    "print(\"\\nTop models based on BIC:\")\n",
    "print(sorted_results_bic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c24425-9dff-426d-a7f3-de24282d9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# Extract the 'Close' series\n",
    "series = dataset_knn_pandas['Close']\n",
    "\n",
    "# Define the train/test split\n",
    "split_point = int(len(series) * 0.8)\n",
    "train, test = series.iloc[:split_point], series.iloc[split_point:]\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted)\n",
    "    return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "# Evaluate ARIMA model for a specific order with a 5-day forecast horizon\n",
    "def evaluate_arima_model(arima_order, forecast_horizon=5):\n",
    "    history = train.tolist()\n",
    "    predictions = []\n",
    "    for t in range(0, len(test), forecast_horizon):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast(steps=forecast_horizon)\n",
    "        predictions.extend(yhat)\n",
    "        history.extend(test.iloc[t:t+forecast_horizon].tolist())\n",
    "    predictions = predictions[:len(test)]\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    r2 = r2_score(test, predictions)\n",
    "    mape = calculate_mape(test, predictions)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Selected ARIMA orders for evaluation\n",
    "selected_orders = [(1, 1, 0), (1, 1, 1)]\n",
    "\n",
    "results = []\n",
    "for order in selected_orders:\n",
    "    rmse, r2, mape = evaluate_arima_model(order, forecast_horizon=5)\n",
    "    results.append({'ARIMA_Order': order, 'RMSE': rmse, 'R2': r2, 'MAPE': mape})\n",
    "    print(f'ARIMA{order} - RMSE: {rmse}, R2: {r2}, MAPE: {mape}')\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame(results).sort_values(by='RMSE')\n",
    "print(\"\\nTop models based on RMSE, R2, and MAPE:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file for later reference\n",
    "results_df.to_csv('arima_model_evaluation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32e5e9-0b54-4b54-83fe-c234b7df7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rebuild the history to include only the training data\n",
    "history = list(train)\n",
    "predictions = []\n",
    "\n",
    "# Generate predictions for the test set only\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(1, 1, 0))\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.forecast()[0]\n",
    "    predictions.append(yhat)\n",
    "    history.append(test.iloc[t])\n",
    "\n",
    "# Create a series for the predicted values with the same index as the test data\n",
    "predicted_series = pd.Series(predictions, index=test.index)\n",
    "\n",
    "# Plot the actual vs predicted values for the entire period (2007-2023)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(series, label='Actual')  # Plot full actual series\n",
    "plt.plot(predicted_series, label='Predicted', color='gray')  # Plot predicted values only in the test period\n",
    "plt.title('ARIMA Model - Actual vs Predicted (2007-2023)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in on the test period to make the predictions more visible\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test.index, test, label='Actual')\n",
    "plt.plot(test.index, predicted_series, label='Predicted', color='red', linestyle='dashed')\n",
    "plt.title('ARIMA Model - Actual vs Predicted (Test Period)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e117607-9043-41b0-b541-e91ddf8e134a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2 AutoArima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332a711-24a8-472d-b89e-ecd86ea7477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# Extract the 'Close' series\n",
    "series = dataset_knn_pandas['Close']\n",
    "\n",
    "# Define the train/test split\n",
    "split_point = int(len(series) * 0.8)\n",
    "train, test = series.iloc[:split_point], series.iloc[split_point:]\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted)\n",
    "    return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "# Use auto_arima to find the best p, d, q values automatically\n",
    "auto_model = auto_arima(train, seasonal=False, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Evaluate the best model found by auto_arima using a 5-day forecast horizon\n",
    "def evaluate_autoarima_model(auto_model, forecast_horizon=5):\n",
    "    history = train.tolist()\n",
    "    predictions = []\n",
    "    for t in range(0, len(test), forecast_horizon):\n",
    "        model_fit = auto_model.fit(history)\n",
    "        yhat = model_fit.predict(n_periods=forecast_horizon)\n",
    "        predictions.extend(yhat)\n",
    "        history.extend(test.iloc[t:t+forecast_horizon].tolist())\n",
    "    predictions = predictions[:len(test)]\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    r2 = r2_score(test, predictions)\n",
    "    mape = calculate_mape(test, predictions)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Evaluate the model\n",
    "rmse, r2, mape = evaluate_autoarima_model(auto_model, forecast_horizon=5)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame([{'ARIMA_Order': auto_model.order, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}])\n",
    "print(\"\\nModel selected by auto_arima:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file for later reference\n",
    "results_df.to_csv('autoarima_model_evaluation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353349a-6e89-4f18-9e86-498aeca416e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the actual vs predicted values for the entire period (2007-2023)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(series, label='Actual')  # Plot full actual series\n",
    "plt.plot(predicted_series, label='Predicted', color='gray', linestyle='dashed')  # Dashed line for predicted values\n",
    "plt.title('ARIMA Model - Actual vs Predicted (2007-2023)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in on the test period to make the predictions more visible\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test.index, test, label='Actual')\n",
    "plt.plot(test.index, predicted_series, label='Predicted', color='red', linestyle='dashed')\n",
    "plt.title('ARIMA Model - Actual vs Predicted (Test Period)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd5447-82b2-4de8-a96e-3909db059fbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3 AutoARIMA with Exogenous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036b379-f451-498a-b16e-4866e2e25310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "def run_model():\n",
    "    # Extract the 'Close' series and the exogenous variables\n",
    "    series = dataset_knn_pandas['Close']\n",
    "    exog_vars = dataset_knn_pandas[['MACD', 'SMA', 'EMA', 'InterestRate', 'InflationRate', \n",
    "                                    'M3', 'M6', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']]\n",
    "\n",
    "    # Define the train/test split\n",
    "    split_point = int(len(series) * 0.8)\n",
    "    train, test = series.iloc[:split_point], series.iloc[split_point:]\n",
    "    train_exog, test_exog = exog_vars.iloc[:split_point], exog_vars.iloc[split_point:]\n",
    "\n",
    "    # Ensure there are no NaN values in the training and test sets, including exogenous variables\n",
    "    train = train.ffill().bfill()\n",
    "    test = test.ffill().bfill()\n",
    "    train_exog = train_exog.ffill().bfill()\n",
    "    test_exog = test_exog.ffill().bfill()\n",
    "\n",
    "    # Verify no NaNs exist after filling\n",
    "    if train.isna().any() or test.isna().any() or train_exog.isna().any().any() or test_exog.isna().any().any():\n",
    "        raise ValueError(\"There are still NaNs in the dataset after filling.\")\n",
    "\n",
    "    # Use auto_arima to find the best p, d, q values automatically with exogenous variables\n",
    "    auto_model = auto_arima(train, X=train_exog, seasonal=False, trace=True, \n",
    "                            error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "    # Fit the model once on the training data\n",
    "    auto_model_fit = auto_model.fit(train, X=train_exog)\n",
    "\n",
    "    # Generate predictions for the test set with a 1-day forecast horizon\n",
    "    predictions = []\n",
    "    forecast_horizon = 1\n",
    "\n",
    "    history = train.copy()\n",
    "    history_exog = train_exog.copy()\n",
    "\n",
    "    for start in range(0, len(test), forecast_horizon):\n",
    "        end = min(start + forecast_horizon, len(test))\n",
    "        pred = auto_model_fit.predict(n_periods=(end - start), X=test_exog.iloc[start:end])\n",
    "        predictions.extend(pred)\n",
    "\n",
    "        # Update the history with the actual test values and exogenous variables using _append()\n",
    "        history = history._append(test.iloc[start:end])\n",
    "        history_exog = history_exog._append(test_exog.iloc[start:end])\n",
    "\n",
    "    # Convert predictions to a Pandas series and align with test data\n",
    "    predicted_series = pd.Series(predictions, index=test.index[:len(predictions)])\n",
    "\n",
    "    # Handle NaNs by forward and backward filling (if still present)\n",
    "    predicted_series = predicted_series.ffill().bfill()\n",
    "\n",
    "    # Final NaN check before evaluation\n",
    "    if predicted_series.isna().any():\n",
    "        print(predicted_series[predicted_series.isna()])\n",
    "        raise ValueError(\"NaNs found in predicted_series after processing.\")\n",
    "\n",
    "    # Ensure the test and predicted series are aligned\n",
    "    test = test[:len(predicted_series)]\n",
    "    predicted_series = predicted_series[-len(test):]\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = sqrt(mean_squared_error(test, predicted_series))\n",
    "    r2 = r2_score(test, predicted_series)\n",
    "    mape = np.mean(np.abs((test - predicted_series) / test)) * 100\n",
    "\n",
    "    # Display the results\n",
    "    results_df = pd.DataFrame([{'ARIMA_Order': auto_model.order, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}])\n",
    "    print(\"\\nModel selected by auto_arima:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Save the results to a CSV file for later reference\n",
    "    results_df.to_csv('autoarima_model_with_exogenous_evaluation_results.csv', index=False)\n",
    "\n",
    "# Profile the function\n",
    "cProfile.run('run_model()', 'output.pstat')\n",
    "\n",
    "# Display the profiling results\n",
    "p = pstats.Stats('output.pstat')\n",
    "p.sort_stats('cumulative').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4710a-2e28-49b5-8f98-427eb38ad619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs predicted values for the entire period (2007-2023)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(series, label='Actual')  # Plot full actual series\n",
    "plt.plot(predicted_series, label='Predicted', color='gray', linestyle='dashed')  # Dashed line for predicted values\n",
    "plt.title('AutoARIMA Model with Exogenous Variables - Actual vs Predicted (2007-2023)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in on the test period to make the predictions more visible\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test.index, test, label='Actual')\n",
    "plt.plot(test.index, predicted_series, label='Predicted', color='red', linestyle='dashed')\n",
    "plt.title('AutoARIMA Model with Exogenous Variables - Actual vs Predicted (Test Period)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d844c2-3c5c-4162-a78b-2dfceb1e8a08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.4 AutoARIMA with Exogenous Variables - Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5331fbe-6dd2-4b1d-8b36-a23ef682f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "import joblib  # For saving the model\n",
    "\n",
    "def run_and_save_model():\n",
    "    # Extract the 'Close' series and the exogenous variables\n",
    "    series = dataset_knn_pandas['Close']\n",
    "    exog_vars = dataset_knn_pandas[['MACD', 'SMA', 'EMA', 'InterestRate', 'InflationRate', \n",
    "                                    'M3', 'M6', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']]\n",
    "\n",
    "    # Define the train/test split\n",
    "    split_point = int(len(series) * 0.8)\n",
    "    train = series.iloc[:split_point]\n",
    "    train_exog = exog_vars.iloc[:split_point]\n",
    "\n",
    "    # Ensure there are no NaN values in the training data, including exogenous variables\n",
    "    train = train.ffill().bfill()\n",
    "    train_exog = train_exog.ffill().bfill()\n",
    "\n",
    "    # Verify no NaNs exist after filling\n",
    "    if train.isna().any() or train_exog.isna().any().any():\n",
    "        raise ValueError(\"There are still NaNs in the training dataset after filling.\")\n",
    "\n",
    "    # Fit the model on the full training data\n",
    "    auto_model = auto_arima(train, X=train_exog, seasonal=False, trace=True, \n",
    "                            error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "    auto_model_fit = auto_model.fit(train, X=train_exog)\n",
    "\n",
    "    # Save the trained ARIMA model\n",
    "    joblib.dump(auto_model_fit, 'arima_model_with_exogenous.pkl')\n",
    "    print(\"Model saved as 'arima_model_with_exogenous.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb868968-c4f9-48d6-a513-5d9d6f89ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_show_results():\n",
    "    # Load the saved ARIMA model\n",
    "    auto_model_fit = joblib.load('arima_model_with_exogenous.pkl')\n",
    "    print(\"Model loaded from 'arima_model_with_exogenous.pkl'\")\n",
    "\n",
    "    # Extract the 'Close' series and the exogenous variables\n",
    "    series = dataset_knn_pandas['Close']\n",
    "    exog_vars = dataset_knn_pandas[['MACD', 'SMA', 'EMA', 'InterestRate', 'InflationRate', \n",
    "                                    'M3', 'M6', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']]\n",
    "\n",
    "    # Define the train/test split\n",
    "    split_point = int(len(series) * 0.8)\n",
    "    test = series.iloc[split_point:]\n",
    "    test_exog = exog_vars.iloc[split_point:]\n",
    "\n",
    "    # Ensure there are no NaN values in the test set, including exogenous variables\n",
    "    test = test.ffill().bfill()\n",
    "    test_exog = test_exog.ffill().bfill()\n",
    "\n",
    "    # Generate predictions for the test set with a 1-day forecast horizon\n",
    "    predictions = []\n",
    "    forecast_horizon = 1\n",
    "\n",
    "    for start in range(0, len(test), forecast_horizon):\n",
    "        end = min(start + forecast_horizon, len(test))\n",
    "        pred = auto_model_fit.predict(n_periods=(end - start), X=test_exog.iloc[start:end])\n",
    "        predictions.extend(pred)\n",
    "\n",
    "    # Convert predictions to a Pandas series and align with test data\n",
    "    predicted_series = pd.Series(predictions, index=test.index[:len(predictions)])\n",
    "\n",
    "    # Handle NaNs by forward and backward filling (if still present)\n",
    "    predicted_series = predicted_series.ffill().bfill()\n",
    "\n",
    "    # Final NaN check before evaluation\n",
    "    if predicted_series.isna().any():\n",
    "        print(predicted_series[predicted_series.isna()])\n",
    "        raise ValueError(\"NaNs found in predicted_series after processing.\")\n",
    "\n",
    "    # Ensure the test and predicted series are aligned\n",
    "    test = test[:len(predicted_series)]\n",
    "    predicted_series = predicted_series[-len(test):]\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = sqrt(mean_squared_error(test, predicted_series))\n",
    "    r2 = r2_score(test, predicted_series)\n",
    "    mape = np.mean(np.abs((test - predicted_series) / test)) * 100\n",
    "\n",
    "    # Display the results\n",
    "    results_df = pd.DataFrame([{'ARIMA_Order': auto_model_fit.order, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}])\n",
    "    print(\"\\nModel selected by auto_arima:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Residual Analysis\n",
    "    residuals = test - predicted_series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(residuals, kde=True, color='blue')\n",
    "    plt.title('Residuals Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(residuals)\n",
    "    plt.title('Residuals Over Time')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the actual vs predicted values for the entire period (2007-2023)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(series, label='Actual')  # Plot full actual series\n",
    "    plt.plot(predicted_series, label='Predicted', color='gray', linestyle='dashed')  # Dashed line for predicted values\n",
    "    plt.title('AutoARIMA Model with Exogenous Variables - Actual vs Predicted (2007-2023)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Zoom in on the test period to make the predictions more visible\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test.index, test, label='Actual')\n",
    "    plt.plot(test.index, predicted_series, label='Predicted', color='red', linestyle='dashed')\n",
    "    plt.title('AutoARIMA Model with Exogenous Variables - Actual vs Predicted (Test Period)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance Analysis (partial)\n",
    "    # Plot the correlation matrix of exogenous variables\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(exog_vars.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Matrix of Exogenous Variables')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe0eca-2db4-4195-8387-b65f30265429",
   "metadata": {},
   "source": [
    "## 4- RIMA with Exogenous Variables RNN (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c49c1-b701-45ef-a7ae-1817200f062d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fc5ab-b078-4c34-a83d-6d6d69fb2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(dataset_knn_pandas):\n",
    "    series = dataset_knn_pandas['Close'].values.reshape(-1, 1)\n",
    "    exog_vars = dataset_knn_pandas[['MACD', 'SMA', 'EMA', 'InterestRate', 'InflationRate', 'M3', 'M6', 'M9', 'Y1', 'Y3', 'Y5', 'Y10', 'Y20', 'Y30']].values\n",
    "    \n",
    "    scaler_series = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_exog = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    series_scaled = scaler_series.fit_transform(series)\n",
    "    exog_vars_scaled = scaler_exog.fit_transform(exog_vars)\n",
    "    \n",
    "    X, y = [], []\n",
    "    window_size = 5\n",
    "    for i in range(window_size, len(series_scaled)):\n",
    "        X.append(np.hstack((series_scaled[i-window_size:i], exog_vars_scaled[i-window_size:i])))\n",
    "        y.append(series_scaled[i])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    split_point = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_point], X[split_point:]\n",
    "    y_train, y_test = y[:split_point], y[split_point:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler_series\n",
    "\n",
    "# Run the data preparation\n",
    "X_train, X_test, y_train, y_test, scaler_series = prepare_data(dataset_knn_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e2a88-5e22-4bcb-ba71-a0746f6292df",
   "metadata": {},
   "source": [
    "### LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5565ac-d715-4606-83d1-967fd5b36704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Create the LSTM model\n",
    "lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355ff5a-93bd-43ff-8d93-f84490efb16e",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f321d-dec0-4886-b56a-3202a63534c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4c749-1d51-40be-9bff-47b277323ea5",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b21c6-d669-4e5e-af64-8e41999bc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the Model with Date Index on X-Axis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# Predict on the test set\n",
    "predicted_stock_price = lstm_model.predict(X_test)\n",
    "predicted_stock_price = scaler_series.inverse_transform(predicted_stock_price)\n",
    "real_stock_price = scaler_series.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = sqrt(mean_squared_error(real_stock_price, predicted_stock_price))\n",
    "r2 = r2_score(real_stock_price, predicted_stock_price)\n",
    "mape = np.mean(np.abs((real_stock_price - predicted_stock_price) / real_stock_price)) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "\n",
    "# Convert the test index back to dates if applicable\n",
    "test_dates = test.index  # Assuming `test` was the original dataframe holding dates\n",
    "\n",
    "# Plot the results with Date Index\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(test_dates, real_stock_price, color='red', label='Real Stock Price')\n",
    "plt.plot(test_dates, predicted_stock_price, color='blue', label='Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction using LSTM')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f160ee-e7c2-4838-b02c-15687e576dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming dataset_knn_pd is already loaded and preprocessed as in the past successful runs\n",
    "# Normalizing the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset_knn_pd[feature_columns])\n",
    "\n",
    "# Function to create a dataset with lagged features\n",
    "def create_dataset(data, look_back=5):  # Updated look_back to 5\n",
    "    X, Y = [], []\n",
    "    target_idx = dataset_knn_pd.columns.get_loc(target_column)\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        a = data[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + look_back, target_idx])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Prepare the dataset with a specified look back period\n",
    "look_back = 30  # 30 Days\n",
    "X, Y = create_dataset(scaled_data, look_back)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.67)\n",
    "trainX, trainY = X[:train_size], Y[:train_size]\n",
    "testX, testY = X[train_size:], Y[train_size:]\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back, len(feature_columns)))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back, len(feature_columns)))\n",
    "\n",
    "# Build and compile the LSTM model (reverting to the successful single-layer structure)\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, len(feature_columns))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "# Save the model\n",
    "model.save('lstm_model.h5')\n",
    "print(\"Model saved as lstm_model.h5\")\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# Invert predictions to get them back to the original scale\n",
    "trainPredict = scaler.inverse_transform(np.column_stack((trainPredict, np.zeros((trainPredict.shape[0], len(feature_columns)-1)))))\n",
    "testPredict = scaler.inverse_transform(np.column_stack((testPredict, np.zeros((testPredict.shape[0], len(feature_columns)-1)))))\n",
    "trainY_original = scaler.inverse_transform(np.column_stack((trainY.reshape(-1, 1), np.zeros((trainY.shape[0], len(feature_columns)-1)))))\n",
    "testY_original = scaler.inverse_transform(np.column_stack((testY.reshape(-1, 1), np.zeros((testY.shape[0], len(feature_columns)-1)))))\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = np.sqrt(mean_squared_error(trainY_original[:, 0], trainPredict[:, 0]))\n",
    "test_rmse = np.sqrt(mean_squared_error(testY_original[:, 0], testPredict[:, 0]))\n",
    "test_mape = np.mean(np.abs((testY_original[:, 0] - testPredict[:, 0]) / testY_original[:, 0])) * 100\n",
    "test_r2 = r2_score(testY_original[:, 0], testPredict[:, 0])\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Train RMSE:', train_rmse)\n",
    "print('Test RMSE:', test_rmse)\n",
    "print('Test MAPE:', test_mape)\n",
    "print('Test R:', test_r2)\n",
    "\n",
    "# Plotting the results with Date Index\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(test.index, testY_original[:, 0], color='red', label='Real Stock Price')\n",
    "plt.plot(test.index, testPredict[:, 0], color='blue', label='Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction using LSTM')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2746c-5951-4c93-8c8f-30a1113bb2ea",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f229c-8afe-4d0d-acf6-ce631ee1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from math import sqrt\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    neurons = trial.suggest_categorical('neurons', [30, 50, 100, 150])\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01, 0.1])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 20, 100)\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['tanh', 'sigmoid'])\n",
    "\n",
    "    # Create the LSTM model with the chosen activation function\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, activation=activation_function, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Cross-validation with time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(trainX):\n",
    "        X_train_fold, X_val_fold = trainX[train_index], trainX[val_index]\n",
    "        y_train_fold, y_val_fold = trainY[train_index], trainY[val_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Evaluate the model\n",
    "        predictions = model.predict(X_val_fold)\n",
    "        rmse_score = np.sqrt(mean_squared_error(y_val_fold, predictions))\n",
    "        rmse_scores.append(rmse_score)\n",
    "\n",
    "    # Return the mean RMSE score over all folds\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Create or load the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print the optimal hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_neurons = trial.params['neurons']\n",
    "best_lr = trial.params['learning_rate']\n",
    "best_batch_size = trial.params['batch_size']\n",
    "best_epochs = trial.params['epochs']\n",
    "best_activation_function = trial.params['activation_function']\n",
    "\n",
    "# Build the final model using the best hyperparameters\n",
    "final_model = Sequential()\n",
    "final_model.add(LSTM(best_neurons, activation=best_activation_function, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(Dense(1))\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_lr), loss='mean_squared_error')\n",
    "\n",
    "# Train the final model\n",
    "history = final_model.fit(trainX, trainY, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
    "\n",
    "# Save the final model\n",
    "final_model.save('best_model.h5')\n",
    "\n",
    "# Predict on the test set\n",
    "predicted_stock_price = final_model.predict(testX).flatten()\n",
    "predicted_stock_price = scaler.inverse_transform(predicted_stock_price.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = sqrt(mean_squared_error(testY, predicted_stock_price))\n",
    "r2 = r2_score(testY, predicted_stock_price)\n",
    "mape = np.mean(np.abs((testY - predicted_stock_price) / testY)) * 100\n",
    "\n",
    "print(f\"Optimized RMSE: {rmse}\")\n",
    "print(f\"Optimized R2 Score: {r2}\")\n",
    "print(f\"Optimized MAPE: {mape}\")\n",
    "\n",
    "# Plotting the results with Date Index\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(test.index, testY_original[:, 0], color='red', label='Real Stock Price')\n",
    "plt.plot(test.index, predicted_stock_price, color='blue', label='Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction using Optimized LSTM')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a42ee-1439-4c5b-bc87-0cda908f6daf",
   "metadata": {},
   "source": [
    "### Retrain the LSTM Model with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a205d-b5ea-428f-9607-b8882a9399ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "\n",
    "# Example of learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "# Building the optimized model with different activation functions\n",
    "def build_lstm_model(input_shape, activation='tanh'):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(units=best_params['n_lstm_units'], activation=activation, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(rate=best_params['dropout_rate']))\n",
    "    model.add(LSTM(units=best_params['n_lstm_units'], activation=activation, return_sequences=False))\n",
    "    model.add(Dropout(rate=best_params['dropout_rate']))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile the model with a learning rate scheduler\n",
    "model = build_lstm_model((X_train.shape[1], X_train.shape[2]), activation='relu')\n",
    "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "# Apply learning rate scheduler and early stopping\n",
    "callbacks = [\n",
    "    LearningRateScheduler(scheduler),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train the model without printing epoch information\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=best_params['batch_size'], validation_split=0.2, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# Predict on the test set\n",
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = scaler_series.inverse_transform(predicted_stock_price).flatten()  # Inverse transform and flatten\n",
    "\n",
    "# Ensure real_stock_price and predicted_stock_price have the same length\n",
    "real_stock_price = real_stock_price[:len(predicted_stock_price)]\n",
    "test_index = test.index[:len(predicted_stock_price)]\n",
    "\n",
    "# Convert real_stock_price to Pandas Series to get the index\n",
    "real_stock_price_series = pd.Series(real_stock_price.flatten(), index=test_index)\n",
    "\n",
    "# Debugging: Print shapes of the arrays\n",
    "print(f\"Shape of real_stock_price_series: {real_stock_price_series.shape}\")\n",
    "print(f\"Shape of predicted_stock_price: {predicted_stock_price.shape}\")\n",
    "\n",
    "# Ensure predicted_stock_price is 1D\n",
    "predicted_stock_price = predicted_stock_price.flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = sqrt(mean_squared_error(real_stock_price_series, predicted_stock_price))\n",
    "r2 = r2_score(real_stock_price_series, predicted_stock_price)\n",
    "mape = np.mean(np.abs((real_stock_price_series - predicted_stock_price) / real_stock_price_series)) * 100\n",
    "\n",
    "print(f\"Optimized RMSE: {rmse}\")\n",
    "print(f\"Optimized R2 Score: {r2}\")\n",
    "print(f\"Optimized MAPE: {mape}\")\n",
    "\n",
    "# Plotting with datetime index\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(real_stock_price_series.index, real_stock_price_series, color='red', label='Real Stock Price')\n",
    "plt.plot(real_stock_price_series.index, predicted_stock_price, color='blue', label='Optimized Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction using Optimized LSTM')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c1551-ddec-4f62-bedc-49d9f4bb16da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Overfitting/Underfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee270a47-304a-4990-99d2-08ad686f020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `history` is the output of the `model.fit` function\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=callbacks, verbose=0)\n",
    "\n",
    "# Plotting Train Loss vs. Validation Loss to check for overfitting/underfitting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Overfitting/Underfitting Analysis')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be341dfa-928f-4f84-9140-b868985eaa2a",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tangent (Tanh & Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592471b-7330-4a0d-916e-41da40938be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Function to build and train the LSTM model\n",
    "def build_and_train_model(X_train, y_train, X_test, activation_function, best_params):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(units=best_params['n_lstm_units'], activation=activation_function, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(rate=best_params['dropout_rate']))\n",
    "    model.add(LSTM(units=best_params['n_lstm_units'], activation=activation_function, return_sequences=False))\n",
    "    model.add(Dropout(rate=best_params['dropout_rate']))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_split=0.2, verbose=0)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    return predictions, model\n",
    "\n",
    "# Parameters\n",
    "best_params = {\n",
    "    'n_lstm_units': 100,  # Adjusted according to your best model\n",
    "    'dropout_rate': 0.2,  # Adjusted according to your best model\n",
    "    'learning_rate': 0.001,  # Adjusted according to your best model\n",
    "    'batch_size': 32,  # Adjusted according to your best model\n",
    "    'epochs': 50  # Adjusted according to your best model\n",
    "}\n",
    "\n",
    "# Train and evaluate using 'tanh' activation\n",
    "pred_tanh, model_tanh = build_and_train_model(X_train, y_train, X_test, 'tanh', best_params)\n",
    "pred_tanh = scaler_series.inverse_transform(pred_tanh).flatten()\n",
    "\n",
    "# Train and evaluate using 'sigmoid' activation\n",
    "pred_sigmoid, model_sigmoid = build_and_train_model(X_train, y_train, X_test, 'sigmoid', best_params)\n",
    "pred_sigmoid = scaler_series.inverse_transform(pred_sigmoid).flatten()\n",
    "\n",
    "# Align real stock prices with predictions\n",
    "real_stock_price_series = pd.Series(real_stock_price.flatten(), index=test.index[:len(pred_tanh)])\n",
    "\n",
    "# Calculate RMSE for both activations\n",
    "rmse_tanh = sqrt(mean_squared_error(real_stock_price_series, pred_tanh))\n",
    "rmse_sigmoid = sqrt(mean_squared_error(real_stock_price_series, pred_sigmoid))\n",
    "\n",
    "print(f\"RMSE with Tanh activation: {rmse_tanh}\")\n",
    "print(f\"RMSE with Sigmoid activation: {rmse_sigmoid}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(real_stock_price_series.index, real_stock_price_series, color='red', label='Real Stock Price')\n",
    "plt.plot(real_stock_price_series.index, pred_tanh, color='blue', label='Predicted with Tanh')\n",
    "plt.plot(real_stock_price_series.index, pred_sigmoid, color='green', label='Predicted with Sigmoid')\n",
    "plt.title('Stock Price Prediction using Different Activation Functions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the activation functions\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "tanh_act = np.tanh(z)\n",
    "sigmoid_act = 1 / (1 + np.exp(-z))\n",
    "\n",
    "def plot_activation_functions():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(z, tanh_act, label='Tanh', color='blue')\n",
    "    plt.plot(z, sigmoid_act, label='Sigmoid', color='red')\n",
    "    plt.title('Comparison of Tanh and Sigmoid Activation Functions')\n",
    "    plt.xlabel('Input value')\n",
    "    plt.ylabel('Activation output')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_activation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7445f-49d6-40d9-b79d-1c690ef1feac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd053e9-9ab6-412d-a4e1-364be4a7b7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
