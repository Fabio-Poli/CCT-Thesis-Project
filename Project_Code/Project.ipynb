{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15954dc3-851c-4a00-bb8a-ac9ebc0f0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, trim\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b58725-ed86-4ec9-9966-98d4c31d29a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date         open         high          low        close  \\\n",
      "0     2024-01-10  4467.150000  4484.800000  4459.540000  4469.490000   \n",
      "1     2024-01-09  4486.990230  4486.990230  4446.620120  4467.169920   \n",
      "2     2024-01-08  4461.799800  4487.259770  4438.490230  4485.479980   \n",
      "3     2024-01-05  4458.740230  4474.879880  4418.080080  4463.509770   \n",
      "4     2024-01-04  4451.720210  4476.720210  4443.419920  4474.009770   \n",
      "...          ...          ...          ...          ...          ...   \n",
      "4870  2004-10-07  2834.620117  2834.620117  2834.620117  2834.620117   \n",
      "4871  2004-10-06  2831.879883  2831.879883  2831.879883  2831.879883   \n",
      "4872  2004-10-05  2830.120117  2830.120117  2830.120117  2830.120117   \n",
      "4873  2004-10-04  2823.459961  2823.459961  2823.459961  2823.459961   \n",
      "4874  2004-10-01  2796.080078  2796.080078  2796.080078  2796.080078   \n",
      "\n",
      "         adjClose    volume  unadjustedVolume    change  changePercent  \\\n",
      "0     4469.490000         0                 0   2.34000       0.052382   \n",
      "1     4467.169920  25544800          25544800 -19.82031      -0.441730   \n",
      "2     4485.479980  21900200          21900200  23.68018       0.530730   \n",
      "3     4463.509770  28778100          28778100   4.76954       0.106970   \n",
      "4     4474.009770  28082800          28082800  22.28956       0.500700   \n",
      "...           ...       ...               ...       ...            ...   \n",
      "4870  2834.620117         0                 0   0.00000       0.000000   \n",
      "4871  2831.879883         0                 0   0.00000       0.000000   \n",
      "4872  2830.120117         0                 0   0.00000       0.000000   \n",
      "4873  2823.459961         0                 0   0.00000       0.000000   \n",
      "4874  2796.080078         0                 0   0.00000       0.000000   \n",
      "\n",
      "         vwap           label  changeOverTime  \n",
      "0     4471.28  January 10, 24        0.000524  \n",
      "1     4471.94  January 09, 24       -0.004417  \n",
      "2     4468.26  January 08, 24        0.005307  \n",
      "3     4453.80  January 05, 24        0.001070  \n",
      "4     4461.47  January 04, 24        0.005007  \n",
      "...       ...             ...             ...  \n",
      "4870     0.00  October 07, 04        0.000000  \n",
      "4871     0.00  October 06, 04        0.000000  \n",
      "4872     0.00  October 05, 04        0.000000  \n",
      "4873     0.00  October 04, 04        0.000000  \n",
      "4874     0.00  October 01, 04        0.000000  \n",
      "\n",
      "[4875 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP historical data endpoint\n",
    "url = f'https://financialmodelingprep.com/api/v3/historical-price-full/{stock_symbol}?from=2004-10-01&to={datetime.now().strftime(\"%Y-%m-%d\")}&apikey={api_key}'\n",
    "\n",
    "# FMP API request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the historical data\n",
    "    if 'historical' in data:\n",
    "        historical_data = data['historical']\n",
    "\n",
    "        # Convert to a pandas DataFrame\n",
    "        index_data = pd.DataFrame(historical_data)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(index_data)\n",
    "    else:\n",
    "        print(\"No 'historical' data found in the response.\")\n",
    "else:\n",
    "    print(f'Failed to retrieve data: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6929b51-4a16-4b09-b3d9-c52b8d384295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>volume</th>\n",
       "      <th>unadjustedVolume</th>\n",
       "      <th>change</th>\n",
       "      <th>changePercent</th>\n",
       "      <th>vwap</th>\n",
       "      <th>changeOverTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4.875000e+03</td>\n",
       "      <td>4.875000e+03</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "      <td>4875.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3312.003310</td>\n",
       "      <td>3332.438368</td>\n",
       "      <td>3289.807229</td>\n",
       "      <td>3311.773624</td>\n",
       "      <td>3311.773006</td>\n",
       "      <td>2.311824e+07</td>\n",
       "      <td>2.312371e+07</td>\n",
       "      <td>-0.229686</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>1873.705252</td>\n",
       "      <td>-0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>585.835219</td>\n",
       "      <td>584.126073</td>\n",
       "      <td>588.232403</td>\n",
       "      <td>586.186874</td>\n",
       "      <td>586.185774</td>\n",
       "      <td>2.570088e+07</td>\n",
       "      <td>2.569879e+07</td>\n",
       "      <td>35.367354</td>\n",
       "      <td>1.166495</td>\n",
       "      <td>1775.478166</td>\n",
       "      <td>0.011665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1812.780029</td>\n",
       "      <td>1823.250000</td>\n",
       "      <td>1765.489990</td>\n",
       "      <td>1809.979981</td>\n",
       "      <td>1809.979981</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-338.020020</td>\n",
       "      <td>-11.724000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2894.760010</td>\n",
       "      <td>2913.244995</td>\n",
       "      <td>2876.134888</td>\n",
       "      <td>2894.619995</td>\n",
       "      <td>2894.619995</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-14.224975</td>\n",
       "      <td>-0.427000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3306.820068</td>\n",
       "      <td>3325.550049</td>\n",
       "      <td>3284.770020</td>\n",
       "      <td>3307.939941</td>\n",
       "      <td>3307.939941</td>\n",
       "      <td>2.266570e+07</td>\n",
       "      <td>2.269050e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2953.073320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3691.865112</td>\n",
       "      <td>3713.640015</td>\n",
       "      <td>3672.700073</td>\n",
       "      <td>3691.640014</td>\n",
       "      <td>3691.640014</td>\n",
       "      <td>4.009285e+07</td>\n",
       "      <td>4.009285e+07</td>\n",
       "      <td>15.820070</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>3476.106655</td>\n",
       "      <td>0.004880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4555.779785</td>\n",
       "      <td>4593.109860</td>\n",
       "      <td>4542.109860</td>\n",
       "      <td>4557.569824</td>\n",
       "      <td>4557.569824</td>\n",
       "      <td>2.358453e+08</td>\n",
       "      <td>2.358453e+08</td>\n",
       "      <td>246.070070</td>\n",
       "      <td>10.076000</td>\n",
       "      <td>4555.620000</td>\n",
       "      <td>0.100760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open         high          low        close     adjClose  \\\n",
       "count  4875.000000  4875.000000  4875.000000  4875.000000  4875.000000   \n",
       "mean   3312.003310  3332.438368  3289.807229  3311.773624  3311.773006   \n",
       "std     585.835219   584.126073   588.232403   586.186874   586.185774   \n",
       "min    1812.780029  1823.250000  1765.489990  1809.979981  1809.979981   \n",
       "25%    2894.760010  2913.244995  2876.134888  2894.619995  2894.619995   \n",
       "50%    3306.820068  3325.550049  3284.770020  3307.939941  3307.939941   \n",
       "75%    3691.865112  3713.640015  3672.700073  3691.640014  3691.640014   \n",
       "max    4555.779785  4593.109860  4542.109860  4557.569824  4557.569824   \n",
       "\n",
       "             volume  unadjustedVolume       change  changePercent  \\\n",
       "count  4.875000e+03      4.875000e+03  4875.000000    4875.000000   \n",
       "mean   2.311824e+07      2.312371e+07    -0.229686      -0.002035   \n",
       "std    2.570088e+07      2.569879e+07    35.367354       1.166495   \n",
       "min    0.000000e+00      0.000000e+00  -338.020020     -11.724000   \n",
       "25%    0.000000e+00      0.000000e+00   -14.224975      -0.427000   \n",
       "50%    2.266570e+07      2.269050e+07     0.000000       0.000000   \n",
       "75%    4.009285e+07      4.009285e+07    15.820070       0.488000   \n",
       "max    2.358453e+08      2.358453e+08   246.070070      10.076000   \n",
       "\n",
       "              vwap  changeOverTime  \n",
       "count  4875.000000     4875.000000  \n",
       "mean   1873.705252       -0.000020  \n",
       "std    1775.478166        0.011665  \n",
       "min       0.000000       -0.117240  \n",
       "25%       0.000000       -0.004270  \n",
       "50%    2953.073320        0.000000  \n",
       "75%    3476.106655        0.004880  \n",
       "max    4555.620000        0.100760  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c0dc72-c92f-4714-9b30-8a4473abf2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date     open      low     high    close  volume  \\\n",
      "0    2024-01-10 13:40:00  4466.31  4464.12  4466.31  4464.23       0   \n",
      "1    2024-01-10 13:35:00  4467.73  4466.35  4467.75  4466.35       0   \n",
      "2    2024-01-10 13:30:00  4471.20  4468.43  4471.97  4468.43       0   \n",
      "3    2024-01-10 13:25:00  4472.60  4471.23  4472.60  4471.44       0   \n",
      "4    2024-01-10 13:20:00  4468.49  4468.49  4472.23  4472.23       0   \n",
      "..                   ...      ...      ...      ...      ...     ...   \n",
      "664  2023-12-29 09:20:00  4531.58  4530.46  4532.42  4532.31       0   \n",
      "665  2023-12-29 09:15:00  4530.36  4530.36  4533.20  4531.77       0   \n",
      "666  2023-12-29 09:10:00  4528.31  4527.86  4530.45  4530.45       0   \n",
      "667  2023-12-29 09:05:00  4527.30  4527.30  4530.42  4528.64       0   \n",
      "668  2023-12-29 09:00:00  4518.12  4518.12  4527.15  4526.70       0   \n",
      "\n",
      "           rsi  \n",
      "0    42.745260  \n",
      "1    46.537637  \n",
      "2    50.630002  \n",
      "3    57.414365  \n",
      "4    59.352648  \n",
      "..         ...  \n",
      "664   0.000000  \n",
      "665   0.000000  \n",
      "666   0.000000  \n",
      "667   0.000000  \n",
      "668   0.000000  \n",
      "\n",
      "[669 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for the stock (e.g., '^STOXX50E' for EURO STOXX 50)\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# Specify the time interval (e.g., '5min', '15min', '1hour', '4hour', '1day')\n",
    "time_interval = '5min'\n",
    "\n",
    "# Specify the period for RSI calculation\n",
    "period = 14  # Commonly used period for RSI\n",
    "\n",
    "# URL for the FMP RSI endpoint\n",
    "url = f'https://financialmodelingprep.com/api/v3/technical_indicator/{time_interval}/{stock_symbol}?type=rsi&period={period}&apikey={api_key}'\n",
    "\n",
    "# FMP API request for RSI data\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    rsi_data = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(rsi_data)\n",
    "else:\n",
    "    print(f'Failed to retrieve RSI data: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597234e-9980-4b87-8cdd-d81bafd89a92",
   "metadata": {},
   "source": [
    "# Interest Rate API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bab679d-b771-4e7e-9a75-f94f69e4a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"ei_mfir_m\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a057cd3-9989-4651-b54c-a3276e121656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'ei_mfir_m.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 's_adj', 'p_adj', 'indic', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['indic'] == 'MF-3MI-RT')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq', 's_adj', 'p_adj'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from October 1, 2004, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2004-10-01 ']\n",
    "structured_data = structured_data[['indic','geo'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f1d191-bc32-4985-9e88-95ab54f6ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Period geo  Interest\n",
      "3    2005-02   EA  2.13840 \n",
      "16   2005-03   EA  2.13720 \n",
      "29   2005-04   EA  2.13720 \n",
      "42   2005-05   EA  2.12560 \n",
      "55   2005-06   EA  2.11100 \n",
      "68   2005-07   EA  2.11940 \n",
      "81   2005-08   EA  2.13250 \n",
      "94   2005-09   EA  2.13910 \n",
      "107  2005-10   EA  2.19660 \n",
      "120  2005-11   EA  2.36090 \n"
     ]
    }
   ],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[5:]  \n",
    "data_for_melting = structured_data[['geo'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "structured_data = pd.melt(data_for_melting, id_vars=['geo'], var_name='Period', value_name='Interest')\n",
    "\n",
    "# Filter for 'geo' == 'EA'\n",
    "Interest_data_ea = structured_data[structured_data['geo'] == 'EA']\n",
    "\n",
    "# Apply forward filling\n",
    "Interest_data_ea.loc[:, 'Interest'] = Interest_data_ea['Interest'].ffill()\n",
    "\n",
    "# Rearrange the columns\n",
    "Interest_data_ea = Interest_data_ea[['Period', 'geo', 'Interest']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(Interest_data_ea.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c56b6d-6827-4427-ad8b-562891a7c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Period geo  Interest\n",
      "0 2005-02-01  EA  2.13840 \n",
      "1 2005-02-02  EA  2.13840 \n",
      "2 2005-02-03  EA  2.13840 \n",
      "3 2005-02-04  EA  2.13840 \n",
      "4 2005-02-05  EA  2.13840 \n",
      "5 2005-02-06  EA  2.13840 \n",
      "6 2005-02-07  EA  2.13840 \n",
      "7 2005-02-08  EA  2.13840 \n",
      "8 2005-02-09  EA  2.13840 \n",
      "9 2005-02-10  EA  2.13840 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/523251794.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Period' to datetime\n",
    "Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n",
    "\n",
    "# Set 'Period' as the index\n",
    "Interest_data_ea.set_index('Period', inplace=True)\n",
    "\n",
    "# Create a daily date range that covers the entire period in your data\n",
    "start_date = Interest_data_ea.index.min()\n",
    "end_date = Interest_data_ea.index.max()\n",
    "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex the DataFrame to this date range, forward filling the missing values\n",
    "daily_interest_data = Interest_data_ea.reindex(all_days).ffill()\n",
    "\n",
    "# Reset the index so 'Period' becomes a column again\n",
    "daily_interest_data.reset_index(inplace=True)\n",
    "daily_interest_data.rename(columns={'index': 'Period'}, inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(daily_interest_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0311287-750a-4045-a788-28f5ca5e55fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Period geo  Interest\n",
      "0 2005-02-01  EA   2.13840\n",
      "1 2005-02-02  EA   2.12988\n",
      "2 2005-02-03  EA   2.12988\n",
      "3 2005-02-04  EA   2.12988\n",
      "4 2005-02-05  EA   2.12988\n",
      "5 2005-02-06  EA   2.12988\n",
      "6 2005-02-07  EA   2.12988\n",
      "7 2005-02-08  EA   2.12988\n",
      "8 2005-02-09  EA   2.12988\n",
      "9 2005-02-10  EA   2.12988\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Period' is in the columns; if not, the index might already be the 'Period'\n",
    "if 'Period' in Interest_data_ea.columns:\n",
    "    Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n",
    "    Interest_data_ea.set_index('Period', inplace=True)\n",
    "elif not isinstance(Interest_data_ea.index, pd.DatetimeIndex):\n",
    "    # If the index is not a datetime, convert it\n",
    "    Interest_data_ea.index = pd.to_datetime(Interest_data_ea.index)\n",
    "\n",
    "# Create a daily date range\n",
    "start_date = Interest_data_ea.index.min()\n",
    "end_date = Interest_data_ea.index.max()\n",
    "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex to create a DataFrame with all days, introducing NaNs for days without data\n",
    "interest_data = Interest_data_ea.reindex(all_days)\n",
    "\n",
    "# Convert the index to a numerical value for KNN\n",
    "interest_data['Day_Ordinal'] = interest_data.index.map(datetime.toordinal)\n",
    "\n",
    "# Extract the 'Interest' column for imputation\n",
    "knn_data = interest_data[['Day_Ordinal', 'Interest']].values\n",
    "\n",
    "# Initialize KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_data_imputed = knn_imputer.fit_transform(knn_data)\n",
    "\n",
    "# Put the imputed 'Interest' data back into the DataFrame\n",
    "interest_data['Interest'] = knn_data_imputed[:, 1]\n",
    "\n",
    "# Drop the 'Day_Ordinal' column as it's no longer needed\n",
    "interest_data.drop('Day_Ordinal', axis=1, inplace=True)\n",
    "\n",
    "# Reset index to make 'Period' a column again\n",
    "interest_data.reset_index(inplace=True)\n",
    "interest_data.rename(columns={'index': 'Period'}, inplace=True)\n",
    "\n",
    "# If the 'geo' column is required, fill it with a constant value (e.g., 'EA')\n",
    "interest_data['geo'] = 'EA'\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(interest_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75932c1a-6e81-4ce1-8f20-a644d6b56323",
   "metadata": {},
   "source": [
    "# Bonds Yields API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c235fd6-cf50-42b0-9142-a4bfd163c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"irt_euryld_d\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3d79b6-dd27-48d4-8ee7-d78e53cb60f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/3711088935.py:10: DtypeWarning: Columns (4417) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'irt_euryld_d.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 'yld_curv', 'maturity', 'bonds', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['yld_curv'] == 'SPOT_RT') & \n",
    "    (structured_data['bonds'] == 'CGB_EA')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq', 'geo'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from October 1, 2004, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2004-10-01 ']\n",
    "structured_data = structured_data[['yld_curv', 'maturity', 'bonds'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eca047b-9076-46cf-a25e-e8496b7bc7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Period maturity     Rate\n",
      "9    2004-10-01        Y1  2.27378\n",
      "141  2004-10-01        Y2  2.58136\n",
      "273  2004-10-01        Y3  2.86602\n",
      "286  2004-10-01        Y4  3.12325\n",
      "298  2004-10-01        Y5  3.35223\n"
     ]
    }
   ],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[3:]  # Assuming date columns start from the 4th column\n",
    "data_for_melting = structured_data[['maturity'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "bonds_data = pd.melt(data_for_melting, id_vars=['maturity'], var_name='Period', value_name='Rate')\n",
    "\n",
    "# Specify the desired maturities (M1 to M12 and Y1 to Y30)\n",
    "desired_maturities = [f'M{i}' for i in range(1, 3)] + [f'Y{i}' for i in range(1, 10)]\n",
    "\n",
    "# Filter the DataFrame to include only the specified maturities\n",
    "bonds_data = bonds_data[bonds_data['maturity'].isin(desired_maturities)]\n",
    "\n",
    "# Rearrange the columns\n",
    "bonds_data = bonds_data[['Period', 'maturity', 'Rate']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(bonds_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2005f-ad08-4054-a554-9c035972bd62",
   "metadata": {},
   "source": [
    "# Inflation Rate API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "155c37e4-15a3-4a40-83f4-912786f10e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"prc_hicp_manr\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7a534f6-4c7e-4161-acc1-e454d0264550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'prc_hicp_manr.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 'unit', 'coicop', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['unit'] == 'RCH_A') & \n",
    "    (structured_data['coicop'] == 'CP00')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from January 1, 2005, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2004-10-01 ']\n",
    "structured_data = structured_data[['unit', 'coicop', 'geo'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87fc8e93-4664-47e4-9e3c-e3ced50311eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Period geo Inflation\n",
      "9    2005-01   EA      1.9 \n",
      "54   2005-02   EA      2.1 \n",
      "99   2005-03   EA      2.2 \n",
      "144  2005-04   EA      2.1 \n",
      "189  2005-05   EA      2.0 \n",
      "234  2005-06   EA      2.0 \n",
      "279  2005-07   EA      2.1 \n",
      "324  2005-08   EA      2.2 \n",
      "369  2005-09   EA      2.6 \n",
      "414  2005-10   EA      2.5 \n"
     ]
    }
   ],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[5:]  # Assuming date columns start from the 6th column\n",
    "data_for_melting = structured_data[['geo'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "inflation_data = pd.melt(data_for_melting, id_vars=['geo'], var_name='Period', value_name='Inflation')\n",
    "\n",
    "# Filter for 'geo' == 'EA'\n",
    "inflation_data_ea = inflation_data[inflation_data['geo'] == 'EA']\n",
    "\n",
    "# Apply forward filling\n",
    "inflation_data_ea.loc[:, 'Inflation'] = inflation_data_ea['Inflation'].ffill()\n",
    "\n",
    "# Rearrange the columns\n",
    "inflation_data_ea = inflation_data_ea[['Period', 'geo', 'Inflation']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(inflation_data_ea.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5f1ae9-ac11-4cec-bbd9-bca717be51c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Period geo Inflation\n",
      "0 2005-01-01  EA      1.9 \n",
      "1 2005-01-02  EA      1.9 \n",
      "2 2005-01-03  EA      1.9 \n",
      "3 2005-01-04  EA      1.9 \n",
      "4 2005-01-05  EA      1.9 \n",
      "5 2005-01-06  EA      1.9 \n",
      "6 2005-01-07  EA      1.9 \n",
      "7 2005-01-08  EA      1.9 \n",
      "8 2005-01-09  EA      1.9 \n",
      "9 2005-01-10  EA      1.9 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/2285021057.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Period' to datetime\n",
    "inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "\n",
    "# Set 'Period' as the DataFrame index\n",
    "inflation_data_ea.set_index('Period', inplace=True)\n",
    "\n",
    "# Resample to daily data using forward fill\n",
    "daily_data_ffill = inflation_data_ea.resample('D').ffill()\n",
    "\n",
    "# Reset index to move 'Period' back to a column\n",
    "daily_data_ffill.reset_index(inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(daily_data_ffill.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a6b469-d155-4ffb-aa15-039b91c124db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Period geo  Inflation\n",
      "0 2005-01-01  EA       1.90\n",
      "1 2005-01-02  EA       2.06\n",
      "2 2005-01-03  EA       2.06\n",
      "3 2005-01-04  EA       2.06\n",
      "4 2005-01-05  EA       2.06\n",
      "5 2005-01-06  EA       2.06\n",
      "6 2005-01-07  EA       2.06\n",
      "7 2005-01-08  EA       2.06\n",
      "8 2005-01-09  EA       2.06\n",
      "9 2005-01-10  EA       2.06\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Period' is in the columns; if not, the index should already be the 'Period'\n",
    "if 'Period' in inflation_data_ea.columns:\n",
    "    inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "    inflation_data_ea.set_index('Period', inplace=True)\n",
    "elif not isinstance(inflation_data_ea.index, pd.DatetimeIndex):\n",
    "    # If the index is not a datetime, convert it\n",
    "    inflation_data_ea.index = pd.to_datetime(inflation_data_ea.index)\n",
    "\n",
    "# Convert non-numeric values to NaN in the 'Inflation' column\n",
    "inflation_data_ea['Inflation'] = pd.to_numeric(inflation_data_ea['Inflation'], errors='coerce')\n",
    "\n",
    "# Create a daily date range that covers the entire period of your data\n",
    "date_range = pd.date_range(start=inflation_data_ea.index.min(), end=inflation_data_ea.index.max(), freq='D')\n",
    "\n",
    "# Create a new DataFrame for KNN imputation\n",
    "inflation_data = pd.DataFrame(date_range, columns=['Period'])\n",
    "\n",
    "# Merge with your existing data - this will introduce NaNs for days without data\n",
    "inflation_data = inflation_data.merge(inflation_data_ea, how='left', left_on='Period', right_index=True)\n",
    "\n",
    "# Convert 'Period' to a numerical value (e.g., days since start)\n",
    "min_date = inflation_data['Period'].min()\n",
    "inflation_data['Day_Ordinal'] = (inflation_data['Period'] - min_date).dt.days\n",
    "\n",
    "# Prepare data for KNN imputation\n",
    "knn_data = inflation_data[['Day_Ordinal', 'Inflation']]\n",
    "\n",
    "# Initialize KNN Imputer - 5 neighbors / Week\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_data_imputed = knn_imputer.fit_transform(knn_data)\n",
    "\n",
    "# Add imputed data back to DataFrame\n",
    "inflation_data['Inflation'] = knn_data_imputed[:, 1]\n",
    "\n",
    "# Fill 'geo' column with 'EA'\n",
    "inflation_data['geo'] = 'EA'\n",
    "\n",
    "# Convert 'Day_Ordinal' back to datetime\n",
    "inflation_data['Period'] = inflation_data['Day_Ordinal'].apply(lambda x: min_date + timedelta(days=x))\n",
    "\n",
    "# Drop the 'Day_Ordinal' column\n",
    "inflation_data.drop(columns='Day_Ordinal', inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(inflation_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf54c7b-3244-4834-82c5-c8b858201b75",
   "metadata": {},
   "source": [
    "# SparkSQL: Tables Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40750bc0-b9d3-4a28-abc9-bd5937c6ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/10 19:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/10 19:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+------------+------------+------------+--------+------------+-------------+\n",
      "|      date|        open|        high|         low|       close|InterestRate|BondRate|BondMaturity|InflationRate|\n",
      "+----------+------------+------------+------------+------------+------------+--------+------------+-------------+\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 4.17243|          Y9|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 4.00003|          Y8|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 3.79492|          Y7|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908|  3.5561|          Y6|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 3.29141|          Y5|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 3.02543|          Y4|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 2.80114|          Y3|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 2.63213|          Y2|          2.9|\n",
      "|2011-10-20|2311.9199219|2324.7700195|2268.0100098|2271.7700195|     1.44908| 2.23963|          Y1|          2.9|\n",
      "|2011-10-21|2283.8500977|2340.8100586|     2279.25|2337.5100098|     1.44908| 4.14155|          Y9|          2.9|\n",
      "+----------+------------+------------+------------+------------+------------+--------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Market Trends Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to format date columns in Pandas DataFrame\n",
    "def format_date_columns(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column]).dt.date\n",
    "    return df\n",
    "\n",
    "# Format date columns\n",
    "index_data = format_date_columns(index_data, 'date')\n",
    "interest_data = format_date_columns(interest_data, 'Period')\n",
    "bonds_data = format_date_columns(bonds_data, 'Period')\n",
    "inflation_data = format_date_columns(inflation_data, 'Period')\n",
    "\n",
    "# Convert the Pandas DataFrames to Spark DataFrames\n",
    "index_df = spark.createDataFrame(index_data)\n",
    "interest_df = spark.createDataFrame(interest_data)\n",
    "bonds_df = spark.createDataFrame(bonds_data)\n",
    "inflation_df = spark.createDataFrame(inflation_data)\n",
    "\n",
    "# Trim and convert date columns to DateType\n",
    "index_df = index_df.withColumn(\"date\", to_date(trim(col(\"date\")), \"yyyy-MM-dd\"))\n",
    "interest_df = interest_df.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "bonds_df = bonds_df.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "inflation_df = inflation_df.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Create Temporary Views\n",
    "index_df.createOrReplaceTempView(\"indexView\")\n",
    "interest_df.createOrReplaceTempView(\"interestView\")\n",
    "bonds_df.createOrReplaceTempView(\"bondsView\")\n",
    "inflation_df.createOrReplaceTempView(\"inflationView\")\n",
    "\n",
    "# SQL Query for Joining\n",
    "join_query = \"\"\"\n",
    "SELECT i.date, i.open, i.high, i.low, i.close, \n",
    "       int.Interest as InterestRate, \n",
    "       b.Rate as BondRate, b.maturity as BondMaturity, \n",
    "       inf.Inflation as InflationRate\n",
    "FROM indexView i\n",
    "LEFT JOIN interestView int ON i.date = int.Period\n",
    "LEFT JOIN bondsView b ON i.date = b.Period\n",
    "LEFT JOIN inflationView inf ON i.date = inf.Period\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "dataset = spark.sql(join_query)\n",
    "\n",
    "# Show the result\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c19e74e-3c2f-493d-9098-de40eb16e475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'date': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'open': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'high': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'low': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'close': Zeros = N/A, Nulls or NaNs = 0\n",
      "Column 'InterestRate': Zeros = N/A, Nulls or NaNs = 967\n",
      "Column 'BondRate': Zeros = N/A, Nulls or NaNs = 17\n",
      "Column 'BondMaturity': Zeros = N/A, Nulls or NaNs = 17\n",
      "Column 'InflationRate': Zeros = N/A, Nulls or NaNs = 778\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "# DataFrame: joined_df\n",
    "numeric_column_types = ['int', 'double', 'float', 'long']\n",
    "\n",
    "# Counting zeros and nulls for each column\n",
    "for column in dataset.columns:\n",
    "    # Check if the column is numeric\n",
    "    if str(dataset.schema[column].dataType) in numeric_column_types:\n",
    "        zero_count = dataset.filter(col(column) == 0).count()\n",
    "        null_or_nan_count = joined_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    else:\n",
    "        zero_count = 'N/A' # Not applicable for non-numeric columns\n",
    "        null_or_nan_count = dataset.filter(col(column).isNull()).count() # Check only for nulls in non-numeric columns\n",
    "\n",
    "    print(f\"Column '{column}': Zeros = {zero_count}, Nulls or NaNs = {null_or_nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac55d381-db6a-41a9-a2bb-006c0bfc4291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- InterestRate: double (nullable = true)\n",
      " |-- BondRate: string (nullable = true)\n",
      " |-- BondMaturity: string (nullable = true)\n",
      " |-- InflationRate: double (nullable = true)\n",
      "\n",
      "Number of rows: 43739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 19:45:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+------------+------------------+\n",
      "|summary|              open|              high|               low|            close|      InterestRate|          BondRate|BondMaturity|     InflationRate|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+------------+------------------+\n",
      "|  count|             43739|             43739|             43739|            43739|             42772|             43722|       43722|             42961|\n",
      "|   mean|3309.9869133101165|3330.4146881369365|3287.7709854644636|3309.717650202538|1.0525140811745326|1.7676058871963647|        NULL|2.1359218826378714|\n",
      "| stddev| 585.0358627009517| 583.3242975406104| 587.4204596863188| 585.359962849712| 1.647108747386468|1.5952702069527223|        NULL| 2.061822031869925|\n",
      "|    min|      1812.7800293|           1823.25|      1765.4899902|     1809.9799805|            -0.582|          -0.00122|          Y1|              -0.6|\n",
      "|    max|      4555.7797852|        4593.10986|        4542.10986|     4557.5698242|            5.1131|            9.7E-4|          Y9|              10.1|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame\n",
    "dataset.printSchema()\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "print(\"Number of rows:\", dataset.count())\n",
    "\n",
    "# Show statistical summaries of the DataFrame\n",
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b22162b-7457-4a60-b26d-4d6eeb6ac8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 43739\n",
      "Rows with 'Open' but missing 'Rate': 17\n",
      "Percentage of Rows with 'Open' but missing 'Rate': 0.04%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "# DataFrame: joined_df\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset.count()\n",
    "\n",
    "# Count rows where 'Open' is available but 'Rate' is missing (Null or NaN)\n",
    "missing_rate_with_open = dataset.filter(\n",
    "    col(\"Rate\").isNull() | isnan(col(\"Rate\")) & col(\"open\").isNotNull()\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_rate_with_open = (missing_rate_with_open / total_rows) * 100\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing 'Rate': {missing_rate_with_open}\")\n",
    "print(f\"Percentage of Rows with 'Open' but missing 'Rate': {percentage_missing_rate_with_open:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dbb69f4-1457-46e5-b0bf-76ae0a8041de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+------------+------------+--------------------+--------+------------+------------------+\n",
      "|      date|        open|        high|         low|       close|        InterestRate|BondRate|BondMaturity|     InflationRate|\n",
      "+----------+------------+------------+------------+------------+--------------------+--------+------------+------------------+\n",
      "|2022-06-22|      3494.0|      3494.0|3413.0400391|3464.6398926|             0.16346|    NULL|        NULL|              8.92|\n",
      "|2020-12-24| 3542.850098|  3546.02002| 3539.790039| 3543.280029| -0.5372800000000001|    NULL|        NULL|               0.5|\n",
      "|2023-04-06|     4295.55|     4313.74|     4293.85|     4309.45|             3.12404|    NULL|        NULL|               6.8|\n",
      "|2022-03-09|3548.4899902|3766.0200195|3548.4899902|3766.0200195|-0.48412000000000005|    NULL|        NULL| 6.780000000000001|\n",
      "|2024-01-09|  4486.99023|  4486.99023|  4446.62012|  4467.16992|                NULL|    NULL|        NULL|              NULL|\n",
      "|2023-12-29|  4518.12012|  4536.43994|  4517.31006|   4521.6499|                NULL|    NULL|        NULL|              NULL|\n",
      "|2023-05-01| 4359.310059| 4359.310059| 4359.310059| 4359.310059|              3.3664|    NULL|        NULL|               6.1|\n",
      "|2023-04-28|     4353.92|      4376.3|     4305.63|     4359.31|  3.3303400000000005|    NULL|        NULL|              6.16|\n",
      "|2020-12-31| 3565.709961| 3571.610107| 3552.639893| 3552.639893| -0.5372800000000001|    NULL|        NULL|0.5000000000000001|\n",
      "|2024-01-10|     4467.15|      4484.8|     4459.54|     4469.49|                NULL|    NULL|        NULL|              NULL|\n",
      "|2021-03-23|3827.5400391|3842.3701172|3804.8999023|3827.0200195|            -0.54028|    NULL|        NULL|1.5399999999999998|\n",
      "|2023-12-22|   4520.6001|   4528.0498|  4505.72998|  4521.47021|                NULL|    NULL|        NULL|              NULL|\n",
      "|2020-05-01| 2927.929932| 2927.929932| 2927.929932| 2927.929932|              -0.272|    NULL|        NULL|               0.1|\n",
      "|2022-04-14|3834.3300781|3859.5900879|3823.8500977|3848.6799316|            -0.41994|    NULL|        NULL|7.4799999999999995|\n",
      "|2021-04-01|3922.3601074|3948.9899902|3922.3601074|3945.9599609|             -0.5382|    NULL|        NULL|               1.6|\n",
      "|2022-12-23|      3823.7|   3833.6101|     3798.74|     3817.01|  2.3569000000000004|    NULL|        NULL|              8.66|\n",
      "|2006-05-01| 3839.899902| 3839.899902| 3839.899902| 3839.899902|               2.889|    NULL|        NULL|               2.5|\n",
      "+----------+------------+------------+------------+------------+--------------------+--------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# DataFrame: joined_df\n",
    "\n",
    "# Filter the DataFrame to get rows where 'Open' is available but 'Rate' is missing (Null or NaN)\n",
    "missing_rate_rows = dataset.filter(\n",
    "    (col(\"Rate\").isNull() | isnan(col(\"Rate\"))) & col(\"open\").isNotNull()\n",
    ")\n",
    "\n",
    "# Show the rows with missing 'Rate' values\n",
    "missing_rate_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8113aa-61d8-449c-876d-ebb9ae4f90ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6f2cf-270c-424a-befa-0e0645596058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
