{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15954dc3-851c-4a00-bb8a-ac9ebc0f0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, trim\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e47dc39-5908-4043-8a78-17d16fc4ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505fd69a-ee90-41e2-ba1e-97f8ff51d0a4",
   "metadata": {},
   "source": [
    "    Importing Libraries: The code starts by importing necessary libraries:\n",
    "        requests for making HTTP requests to the FMP API.\n",
    "        pandas for data manipulation and analysis.\n",
    "        datetime for handling dates and times.\n",
    "\n",
    "    API Key and Stock Symbol:\n",
    "        An API key for FMP is defined. This key is essential for authentication and accessing the data from FMP.\n",
    "        The stock symbol ^STOXX50E for the EURO STOXX 50 index is specified.\n",
    "\n",
    "    Building the API Request URL:\n",
    "        The URL for the FMP API is constructed using string formatting. It includes the stock symbol, a date range (from February 1, 2005, to the current date), and the API key.\n",
    "\n",
    "    Making the API Request:\n",
    "        A GET request is made to the constructed URL using the requests library.\n",
    "        The response is checked for a successful status code (200). If the request is successful, the data is processed; if not, an error message is displayed.\n",
    "\n",
    "    Data Processing:\n",
    "        The successful response is converted from JSON format to a Python dictionary.\n",
    "        The code checks for the presence of the 'historical' key in the data, which contains the required historical stock price data.\n",
    "        This historical data is then converted into a pandas DataFrame for easier manipulation and analysis.\n",
    "\n",
    "    Filtering and Displaying Data:\n",
    "        The DataFrame is filtered to include only the 'date' and 'close' columns, representing the closing prices of the stock on each date.\n",
    "        Finally, the filtered DataFrame is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b58725-ed86-4ec9-9966-98d4c31d29a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date        close\n",
      "0     2024-01-26  4635.470220\n",
      "1     2024-01-25  4582.259770\n",
      "2     2024-01-24  4564.109860\n",
      "3     2024-01-23  4465.910160\n",
      "4     2024-01-22  4480.319820\n",
      "...          ...          ...\n",
      "1016  2020-02-07  3798.489990\n",
      "1017  2020-02-06  3805.520020\n",
      "1018  2020-02-05  3777.840088\n",
      "1019  2020-02-04  3732.280029\n",
      "1020  2020-02-03  3661.270020\n",
      "\n",
      "[1021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP historical data endpoint\n",
    "url = f'https://financialmodelingprep.com/api/v3/historical-price-full/{stock_symbol}?from=2020-02-01&to={datetime.now().strftime(\"%Y-%m-%d\")}&apikey={api_key}'\n",
    "\n",
    "# FMP API request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the historical data\n",
    "    if 'historical' in data:\n",
    "        historical_data = data['historical']\n",
    "\n",
    "        # Convert to a pandas DataFrame\n",
    "        index_df = pd.DataFrame(historical_data)\n",
    "\n",
    "        # Filter the DataFrame to only include the 'Close' column\n",
    "        index_df = index_df[['date', 'close']]\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(index_df)\n",
    "    else:\n",
    "        print(\"No 'historical' data found in the response.\")\n",
    "else:\n",
    "    print(f'Failed to retrieve data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd61ccf-791e-4a71-9afd-c48e59c71307",
   "metadata": {},
   "source": [
    "## Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97393cbe-8e2e-4449-b1c5-012842a8054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date          sma\n",
      "0     2024-01-26  4460.275762\n",
      "1     2024-01-25  4453.400762\n",
      "2     2024-01-24  4446.399365\n",
      "3     2024-01-23  4439.064365\n",
      "4     2024-01-22  4434.330166\n",
      "...          ...          ...\n",
      "1016  2020-02-07  3733.441206\n",
      "1017  2020-02-06  3731.064604\n",
      "1018  2020-02-05  3728.631802\n",
      "1019  2020-02-04  3727.006201\n",
      "1020  2020-02-03  3726.458999\n",
      "\n",
      "[1021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP SMA technical indicator endpoint with daily data\n",
    "sma_period = 50\n",
    "url = f'https://financialmodelingprep.com/api/v3/technical_indicator/daily/{stock_symbol}?period={sma_period}&type=sma&apikey={api_key}'\n",
    "\n",
    "# FMP API request for SMA data\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    sma_data = response.json()\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    sma_df = pd.DataFrame(sma_data)\n",
    "\n",
    "    # Ensure 'date' column is in datetime format\n",
    "    sma_df['date'] = pd.to_datetime(sma_df['date'])\n",
    "\n",
    "    # Format the 'date' column to display dates only\n",
    "    sma_df['date'] = sma_df['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Filter the DataFrame for rows where the date is after February 1, 2005\n",
    "    sma_df = sma_df[sma_df['date'] >= '2020-02-01']\n",
    "\n",
    "    # Keep only the 'date' and 'sma' columns\n",
    "    sma_df = sma_df[['date', 'sma']]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(sma_df)\n",
    "else:\n",
    "    print(f'Failed to retrieve SMA data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2817200f-7d4f-4c5f-9af1-e7fe43ebc456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date          ema\n",
      "0     2024-01-26  4440.930772\n",
      "1     2024-01-25  4432.990386\n",
      "2     2024-01-24  4426.897758\n",
      "3     2024-01-23  4421.297264\n",
      "4     2024-01-22  4419.476330\n",
      "...          ...          ...\n",
      "1016  2020-02-07  3724.103977\n",
      "1017  2020-02-06  3721.067813\n",
      "1018  2020-02-05  3717.620784\n",
      "1019  2020-02-04  3715.162853\n",
      "1020  2020-02-03  3714.464193\n",
      "\n",
      "[1021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# FMP API key\n",
    "api_key = '47461a32a9bcf3b0a6c2c6e98ccfab42'\n",
    "\n",
    "# Symbol for EURO STOXX 50\n",
    "stock_symbol = '^STOXX50E'\n",
    "\n",
    "# URL for the FMP EMA technical indicator endpoint with daily data\n",
    "ema_period = 50\n",
    "url = f'https://financialmodelingprep.com/api/v3/technical_indicator/daily/{stock_symbol}?period={ema_period}&type=ema&apikey={api_key}'\n",
    "\n",
    "# FMP API request for EMA data\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response to JSON\n",
    "    ema_data = response.json()\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    ema_df = pd.DataFrame(ema_data)\n",
    "\n",
    "    # Ensure 'date' column is in datetime format\n",
    "    ema_df['date'] = pd.to_datetime(ema_df['date'])\n",
    "\n",
    "    # Format the 'date' column to display dates only\n",
    "    ema_df['date'] = ema_df['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Filter the DataFrame for rows where the date is after February 1, 2005\n",
    "    ema_df = ema_df[ema_df['date'] >= '2020-02-01']\n",
    "\n",
    "    # Keep only the 'date' and 'ema' columns\n",
    "    ema_df = ema_df[['date', 'ema']]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(ema_df)\n",
    "else:\n",
    "    print(f'Failed to retrieve EMA data: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12127d58-5bde-4db3-b798-8f2fbeff4644",
   "metadata": {},
   "source": [
    "# Index + Technical Indicator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d19e0d-e4a0-4bdf-81f1-2a3e0087729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date        Close          SMA          EMA\n",
      "0     2024-01-26  4635.470220  4460.275762  4440.930772\n",
      "1     2024-01-25  4582.259770  4453.400762  4432.990386\n",
      "2     2024-01-24  4564.109860  4446.399365  4426.897758\n",
      "3     2024-01-23  4465.910160  4439.064365  4421.297264\n",
      "4     2024-01-22  4480.319820  4434.330166  4419.476330\n",
      "...          ...          ...          ...          ...\n",
      "1016  2020-02-07  3798.489990  3733.441206  3724.103977\n",
      "1017  2020-02-06  3805.520020  3731.064604  3721.067813\n",
      "1018  2020-02-05  3777.840088  3728.631802  3717.620784\n",
      "1019  2020-02-04  3732.280029  3727.006201  3715.162853\n",
      "1020  2020-02-03  3661.270020  3726.458999  3714.464193\n",
      "\n",
      "[1021 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge index_df and sma_df on 'date'\n",
    "merged_df = pd.merge(index_df, sma_df, on='date', how='left')\n",
    "\n",
    "# Merge the result with ema_df on 'date'\n",
    "index_data = pd.merge(merged_df, ema_df, on='date', how='left')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "index_data.columns = ['Date', 'Close', 'SMA', 'EMA']\n",
    "\n",
    "# Display the DataFrame\n",
    "print(index_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a953f7-8542-44b8-b0c4-8931952aca68",
   "metadata": {},
   "source": [
    "## Removed SMA & EMA (API Problem Historic Data). Might back it when the API is ready again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6929b51-4a16-4b09-b3d9-c52b8d384295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1021.000000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>1021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3844.347609</td>\n",
       "      <td>3826.418653</td>\n",
       "      <td>3826.967357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>449.201569</td>\n",
       "      <td>413.782893</td>\n",
       "      <td>406.245890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2385.820068</td>\n",
       "      <td>2787.037207</td>\n",
       "      <td>2937.739941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3517.250000</td>\n",
       "      <td>3556.154810</td>\n",
       "      <td>3530.651596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3945.959961</td>\n",
       "      <td>3888.129805</td>\n",
       "      <td>3898.835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4209.150000</td>\n",
       "      <td>4197.753394</td>\n",
       "      <td>4195.106706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4635.470220</td>\n",
       "      <td>4460.275762</td>\n",
       "      <td>4440.930772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close          SMA          EMA\n",
       "count  1021.000000  1021.000000  1021.000000\n",
       "mean   3844.347609  3826.418653  3826.967357\n",
       "std     449.201569   413.782893   406.245890\n",
       "min    2385.820068  2787.037207  2937.739941\n",
       "25%    3517.250000  3556.154810  3530.651596\n",
       "50%    3945.959961  3888.129805  3898.835821\n",
       "75%    4209.150000  4197.753394  4195.106706\n",
       "max    4635.470220  4460.275762  4440.930772"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597234e-9980-4b87-8cdd-d81bafd89a92",
   "metadata": {},
   "source": [
    "# Interest Rate API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bab679d-b771-4e7e-9a75-f94f69e4a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n",
      "Dataset unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"ei_mfir_m\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a057cd3-9989-4651-b54c-a3276e121656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/fabiopoli/Desktop\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'ei_mfir_m.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 's_adj', 'p_adj', 'indic', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['indic'] == 'MF-3MI-RT')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq', 's_adj', 'p_adj'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from October 1, 2004, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2005-02-01 ']\n",
    "structured_data = structured_data[['indic','geo'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f1d191-bc32-4985-9e88-95ab54f6ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Period geo  Interest\n",
      "3    2005-06   EA  2.11100 \n",
      "16   2005-07   EA  2.11940 \n",
      "29   2005-08   EA  2.13250 \n",
      "42   2005-09   EA  2.13910 \n",
      "55   2005-10   EA  2.19660 \n",
      "68   2005-11   EA  2.36090 \n",
      "81   2005-12   EA  2.47290 \n",
      "94   2006-01   EA  2.51170 \n",
      "107  2006-02   EA  2.60040 \n",
      "120  2006-03   EA  2.72260 \n"
     ]
    }
   ],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[5:]  \n",
    "data_for_melting = structured_data[['geo'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "structured_data = pd.melt(data_for_melting, id_vars=['geo'], var_name='Period', value_name='Interest')\n",
    "\n",
    "# Filter for 'geo' == 'EA'\n",
    "Interest_data_ea = structured_data[structured_data['geo'] == 'EA']\n",
    "\n",
    "# Apply forward filling\n",
    "Interest_data_ea.loc[:, 'Interest'] = Interest_data_ea['Interest'].ffill()\n",
    "\n",
    "# Rearrange the columns\n",
    "Interest_data_ea = Interest_data_ea[['Period', 'geo', 'Interest']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(Interest_data_ea.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24c56b6d-6827-4427-ad8b-562891a7c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Period geo  Interest\n",
      "0 2005-06-01  EA  2.11100 \n",
      "1 2005-06-02  EA  2.11100 \n",
      "2 2005-06-03  EA  2.11100 \n",
      "3 2005-06-04  EA  2.11100 \n",
      "4 2005-06-05  EA  2.11100 \n",
      "5 2005-06-06  EA  2.11100 \n",
      "6 2005-06-07  EA  2.11100 \n",
      "7 2005-06-08  EA  2.11100 \n",
      "8 2005-06-09  EA  2.11100 \n",
      "9 2005-06-10  EA  2.11100 \n"
     ]
    }
   ],
   "source": [
    "# Convert 'Period' to datetime\n",
    "Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n",
    "\n",
    "# Set 'Period' as the index\n",
    "Interest_data_ea.set_index('Period', inplace=True)\n",
    "\n",
    "# Create a daily date range that covers the entire period in your data\n",
    "start_date = Interest_data_ea.index.min()\n",
    "end_date = Interest_data_ea.index.max()\n",
    "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex the DataFrame to this date range, forward filling the missing values\n",
    "daily_interest_data = Interest_data_ea.reindex(all_days).ffill()\n",
    "\n",
    "# Reset the index so 'Period' becomes a column again\n",
    "daily_interest_data.reset_index(inplace=True)\n",
    "daily_interest_data.rename(columns={'index': 'Period'}, inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(daily_interest_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0311287-750a-4045-a788-28f5ca5e55fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Period geo  Interest\n",
      "0 2005-06-01  EA   2.11100\n",
      "1 2005-06-02  EA   2.13972\n",
      "2 2005-06-03  EA   2.13972\n",
      "3 2005-06-04  EA   2.13972\n",
      "4 2005-06-05  EA   2.13972\n",
      "5 2005-06-06  EA   2.13972\n",
      "6 2005-06-07  EA   2.13972\n",
      "7 2005-06-08  EA   2.13972\n",
      "8 2005-06-09  EA   2.13972\n",
      "9 2005-06-10  EA   2.13972\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Period' is in the columns; if not, the index might already be the 'Period'\n",
    "if 'Period' in Interest_data_ea.columns:\n",
    "    Interest_data_ea['Period'] = pd.to_datetime(Interest_data_ea['Period'])\n",
    "    Interest_data_ea.set_index('Period', inplace=True)\n",
    "elif not isinstance(Interest_data_ea.index, pd.DatetimeIndex):\n",
    "    # If the index is not a datetime, convert it\n",
    "    Interest_data_ea.index = pd.to_datetime(Interest_data_ea.index)\n",
    "\n",
    "# Create a daily date range\n",
    "start_date = Interest_data_ea.index.min()\n",
    "end_date = Interest_data_ea.index.max()\n",
    "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex to create a DataFrame with all days, introducing NaNs for days without data\n",
    "interest_data = Interest_data_ea.reindex(all_days)\n",
    "\n",
    "# Convert the index to a numerical value for KNN\n",
    "interest_data['Day_Ordinal'] = interest_data.index.map(datetime.toordinal)\n",
    "\n",
    "# Extract the 'Interest' column for imputation\n",
    "knn_data = interest_data[['Day_Ordinal', 'Interest']].values\n",
    "\n",
    "# Initialize KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_data_imputed = knn_imputer.fit_transform(knn_data)\n",
    "\n",
    "# Put the imputed 'Interest' data back into the DataFrame\n",
    "interest_data['Interest'] = knn_data_imputed[:, 1]\n",
    "\n",
    "# Drop the 'Day_Ordinal' column as it's no longer needed\n",
    "interest_data.drop('Day_Ordinal', axis=1, inplace=True)\n",
    "\n",
    "# Reset index to make 'Period' a column again\n",
    "interest_data.reset_index(inplace=True)\n",
    "interest_data.rename(columns={'index': 'Period'}, inplace=True)\n",
    "\n",
    "# If the 'geo' column is required, fill it with a constant value (e.g., 'EA')\n",
    "interest_data['geo'] = 'EA'\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(interest_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75932c1a-6e81-4ce1-8f20-a644d6b56323",
   "metadata": {},
   "source": [
    "# Bonds Yields API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c235fd6-cf50-42b0-9142-a4bfd163c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"irt_euryld_d\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d79b6-dd27-48d4-8ee7-d78e53cb60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'irt_euryld_d.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 'yld_curv', 'maturity', 'bonds', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['yld_curv'] == 'SPOT_RT') & \n",
    "    (structured_data['bonds'] == 'CGB_EA')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq', 'geo'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from October 1, 2004, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2004-10-01 ']\n",
    "structured_data = structured_data[['yld_curv', 'maturity', 'bonds'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca047b-9076-46cf-a25e-e8496b7bc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[3:]  # Assuming date columns start from the 4th column\n",
    "data_for_melting = structured_data[['maturity'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "bonds_data = pd.melt(data_for_melting, id_vars=['maturity'], var_name='Period', value_name='Rate')\n",
    "\n",
    "# Specify the desired maturities (M1 to M12 and Y1 to Y30)\n",
    "desired_maturities = [f'M{i}' for i in range(1, 3)] + [f'Y{i}' for i in range(1, 10)]\n",
    "\n",
    "# Filter the DataFrame to include only the specified maturities\n",
    "bonds_data = bonds_data[bonds_data['maturity'].isin(desired_maturities)]\n",
    "\n",
    "# Rearrange the columns\n",
    "bonds_data = bonds_data[['Period', 'maturity', 'Rate']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(bonds_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2005f-ad08-4054-a554-9c035972bd62",
   "metadata": {},
   "source": [
    "# Inflation Rate API (Eurostat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c37e4-15a3-4a40-83f4-912786f10e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset code\n",
    "dataset_code = \"prc_hicp_manr\"\n",
    "\n",
    "# API URL for downloading the dataset\n",
    "api_url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/{dataset_code}?format=TSV&compressed=true\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(api_url, stream=True)\n",
    "output_file = dataset_code + \".tsv.gz\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error downloading dataset. Status code: {response.status_code}\")\n",
    "\n",
    "# Unzipping the dataset\n",
    "with gzip.open(output_file, 'rb') as f_in:\n",
    "    with open(dataset_code + \".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Dataset unzipped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a534f6-4c7e-4161-acc1-e454d0264550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Construct the file path dynamically\n",
    "file_name = 'prc_hicp_manr.tsv'  # File Name\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Split the first column into separate columns\n",
    "split_columns = data.iloc[:, 0].str.split(',', expand=True)\n",
    "split_columns.columns = ['freq', 'unit', 'coicop', 'geo']\n",
    "\n",
    "# Combine the split columns with the original data (excluding the first column)\n",
    "structured_data = pd.concat([split_columns, data.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Apply filters\n",
    "structured_data = structured_data[\n",
    "    (structured_data['unit'] == 'RCH_A') & \n",
    "    (structured_data['coicop'] == 'CP00')\n",
    "]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "structured_data.drop(['freq'], axis=1, inplace=True)\n",
    "\n",
    "# Filter for date range (from January 1, 2005, onwards)\n",
    "date_columns = structured_data.columns[3:]\n",
    "dates_from_oct_2004 = date_columns[date_columns >= '2005-02-01 ']\n",
    "structured_data = structured_data[['unit', 'coicop', 'geo'] + list(dates_from_oct_2004)]\n",
    "\n",
    "# Reset the index for a clean table\n",
    "structured_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the structured DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc8e93-4664-47e4-9e3c-e3ced50311eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only the columns with date values for melting\n",
    "date_columns = structured_data.columns[5:]  # Assuming date columns start from the 6th column\n",
    "data_for_melting = structured_data[['geo'] + list(date_columns)]\n",
    "\n",
    "# Melt the DataFrame to change its structure\n",
    "inflation_data = pd.melt(data_for_melting, id_vars=['geo'], var_name='Period', value_name='Inflation')\n",
    "\n",
    "# Filter for 'geo' == 'EA'\n",
    "inflation_data_ea = inflation_data[inflation_data['geo'] == 'EA']\n",
    "\n",
    "# Apply forward filling\n",
    "inflation_data_ea.loc[:, 'Inflation'] = inflation_data_ea['Inflation'].ffill()\n",
    "\n",
    "# Rearrange the columns\n",
    "inflation_data_ea = inflation_data_ea[['Period', 'geo', 'Inflation']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "pd.set_option('display.max_rows', None)  # This option displays all the rows\n",
    "print(inflation_data_ea.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f1ae9-ac11-4cec-bbd9-bca717be51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Period' to datetime\n",
    "inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "\n",
    "# Set 'Period' as the DataFrame index\n",
    "inflation_data_ea.set_index('Period', inplace=True)\n",
    "\n",
    "# Resample to daily data using forward fill\n",
    "daily_data_ffill = inflation_data_ea.resample('D').ffill()\n",
    "\n",
    "# Reset index to move 'Period' back to a column\n",
    "daily_data_ffill.reset_index(inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(daily_data_ffill.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6b469-d155-4ffb-aa15-039b91c124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'Period' is in the columns; if not, the index should already be the 'Period'\n",
    "if 'Period' in inflation_data_ea.columns:\n",
    "    inflation_data_ea['Period'] = pd.to_datetime(inflation_data_ea['Period'])\n",
    "    inflation_data_ea.set_index('Period', inplace=True)\n",
    "elif not isinstance(inflation_data_ea.index, pd.DatetimeIndex):\n",
    "    # If the index is not a datetime, convert it\n",
    "    inflation_data_ea.index = pd.to_datetime(inflation_data_ea.index)\n",
    "\n",
    "# Convert non-numeric values to NaN in the 'Inflation' column\n",
    "inflation_data_ea['Inflation'] = pd.to_numeric(inflation_data_ea['Inflation'], errors='coerce')\n",
    "\n",
    "# Create a daily date range that covers the entire period of your data\n",
    "date_range = pd.date_range(start=inflation_data_ea.index.min(), end=inflation_data_ea.index.max(), freq='D')\n",
    "\n",
    "# Create a new DataFrame for KNN imputation\n",
    "inflation_data = pd.DataFrame(date_range, columns=['Period'])\n",
    "\n",
    "# Merge with your existing data - this will introduce NaNs for days without data\n",
    "inflation_data = inflation_data.merge(inflation_data_ea, how='left', left_on='Period', right_index=True)\n",
    "\n",
    "# Convert 'Period' to a numerical value (e.g., days since start)\n",
    "min_date = inflation_data['Period'].min()\n",
    "inflation_data['Day_Ordinal'] = (inflation_data['Period'] - min_date).dt.days\n",
    "\n",
    "# Prepare data for KNN imputation\n",
    "knn_data = inflation_data[['Day_Ordinal', 'Inflation']]\n",
    "\n",
    "# Initialize KNN Imputer - 5 neighbors / Week\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_data_imputed = knn_imputer.fit_transform(knn_data)\n",
    "\n",
    "# Add imputed data back to DataFrame\n",
    "inflation_data['Inflation'] = knn_data_imputed[:, 1]\n",
    "\n",
    "# Fill 'geo' column with 'EA'\n",
    "inflation_data['geo'] = 'EA'\n",
    "\n",
    "# Convert 'Day_Ordinal' back to datetime\n",
    "inflation_data['Period'] = inflation_data['Day_Ordinal'].apply(lambda x: min_date + timedelta(days=x))\n",
    "\n",
    "# Drop the 'Day_Ordinal' column\n",
    "inflation_data.drop(columns='Day_Ordinal', inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the daily data\n",
    "print(inflation_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf54c7b-3244-4834-82c5-c8b858201b75",
   "metadata": {},
   "source": [
    "# SparkSQL: Tables Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40750bc0-b9d3-4a28-abc9-bd5937c6ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Market Trends Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to format date columns in Pandas DataFrame\n",
    "def format_date_columns(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column]).dt.date\n",
    "    return df\n",
    "\n",
    "# Format date columns\n",
    "index_data = format_date_columns(index_data, 'Date')\n",
    "interest_data = format_date_columns(interest_data, 'Period')\n",
    "bonds_data = format_date_columns(bonds_data, 'Period')\n",
    "inflation_data = format_date_columns(inflation_data, 'Period')\n",
    "\n",
    "# Convert the Pandas DataFrames to Spark DataFrames\n",
    "index_df = spark.createDataFrame(index_data)\n",
    "interest_df = spark.createDataFrame(interest_data)\n",
    "bonds_df = spark.createDataFrame(bonds_data)\n",
    "inflation_df = spark.createDataFrame(inflation_data)\n",
    "\n",
    "# Trim and convert date columns to DateType\n",
    "index_df = index_df.withColumn(\"date\", to_date(trim(col(\"date\")), \"yyyy-MM-dd\"))\n",
    "interest_df = interest_df.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "bonds_df = bonds_df.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "inflation_df = inflation_df.withColumn(\"Period\", to_date(trim(col(\"Period\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Create Temporary Views\n",
    "index_df.createOrReplaceTempView(\"indexView\")\n",
    "interest_df.createOrReplaceTempView(\"interestView\")\n",
    "bonds_df.createOrReplaceTempView(\"bondsView\")\n",
    "inflation_df.createOrReplaceTempView(\"inflationView\")\n",
    "\n",
    "# SQL Query for Joining\n",
    "join_query = \"\"\"\n",
    "SELECT i.Date, i.Close, i.SMA, i.EMA,\n",
    "       int.Interest as InterestRate, \n",
    "       b.Rate as BondRate, b.maturity as BondMaturity, \n",
    "       inf.Inflation as InflationRate\n",
    "FROM indexView i\n",
    "LEFT JOIN interestView int ON i.date = int.Period\n",
    "LEFT JOIN bondsView b ON i.date = b.Period\n",
    "LEFT JOIN inflationView inf ON i.date = inf.Period\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "dataset = spark.sql(join_query)\n",
    "\n",
    "# Show the result\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19e74e-3c2f-493d-9098-de40eb16e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame: joined_df\n",
    "numeric_column_types = ['int', 'double', 'float', 'long']\n",
    "\n",
    "# Counting zeros and nulls for each column\n",
    "for column in dataset.columns:\n",
    "    # Check if the column is numeric\n",
    "    if str(dataset.schema[column].dataType) in numeric_column_types:\n",
    "        zero_count = dataset.filter(col(column) == 0).count()\n",
    "        null_or_nan_count = joined_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    else:\n",
    "        zero_count = 'N/A' # Not applicable for non-numeric columns\n",
    "        null_or_nan_count = dataset.filter(col(column).isNull()).count() # Check only for nulls in non-numeric columns\n",
    "\n",
    "    print(f\"Column '{column}': Zeros = {zero_count}, Nulls or NaNs = {null_or_nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55d381-db6a-41a9-a2bb-006c0bfc4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema of the DataFrame\n",
    "dataset.printSchema()\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "print(\"Number of rows:\", dataset.count())\n",
    "\n",
    "# Show statistical summaries of the DataFrame\n",
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22162b-7457-4a60-b26d-4d6eeb6ac8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of rows\n",
    "total_rows = dataset.count()\n",
    "\n",
    "# Count rows where specific columns are available but others are missing (Null or NaN)\n",
    "# Adjust column names as per your DataFrame\n",
    "missing_columns_count = dataset.filter(\n",
    "    (col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\"))) &\n",
    "    (col(\"BondRate\").isNull() | isnan(col(\"BondRate\"))) &\n",
    "    (col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))) &\n",
    "    col(\"Close\").isNotNull()\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_columns = (missing_columns_count / total_rows) * 100\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing 'InterestRate', 'BondRate', and 'InflationRate': {missing_columns_count}\")\n",
    "print(f\"Percentage of Rows with missing rates: {percentage_missing_columns:.2f}%\")\n",
    "\n",
    "# Filter the DataFrame to get rows with missing 'InterestRate', 'BondRate', and 'InflationRate' but available 'Open'\n",
    "missing_values_table = dataset.filter(\n",
    "    (col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\"))) &\n",
    "    (col(\"BondRate\").isNull() | isnan(col(\"BondRate\"))) &\n",
    "    (col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))) &\n",
    "    col(\"Close\").isNotNull()\n",
    ")\n",
    "\n",
    "# Show the rows with missing values\n",
    "missing_values_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb69f4-1457-46e5-b0bf-76ae0a8041de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of rows\n",
    "total_rows = dataset.count()\n",
    "\n",
    "# Count rows where any one of the specific columns is available but others are missing (Null or NaN)\n",
    "# Adjust column names as per your DataFrame\n",
    "missing_any_column_count = dataset.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"BondRate\").isNull() | isnan(col(\"BondRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing any one of 'InterestRate', 'BondRate', or 'InflationRate': {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing any one rate: {percentage_missing_any_column:.2f}%\")\n",
    "\n",
    "# Filter the DataFrame to get rows with missing any one of 'InterestRate', 'BondRate', and 'InflationRate' but available 'Open'\n",
    "missing_any_values_table = dataset.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"BondRate\").isNull() | isnan(col(\"BondRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the rows with missing values\n",
    "missing_any_values_table.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97a36b-25c6-41ba-8fe3-d39c9f0d50a4",
   "metadata": {},
   "source": [
    "## Window Approuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8113aa-61d8-449c-876d-ebb9ae4f90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last\n",
    "\n",
    "# Define the window specification for forward fill with partitioning by 'date'\n",
    "window_spec = Window.partitionBy(\"Date\").orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Apply forward fill for each column that needs imputation\n",
    "dataset_ff = dataset.withColumn(\"InterestRate\", last(\"InterestRate\", ignorenulls=True).over(window_spec)) \\\n",
    "                    .withColumn(\"BondRate\", last(\"BondRate\", ignorenulls=True).over(window_spec)) \\\n",
    "                    .withColumn(\"InflationRate\", last(\"InflationRate\", ignorenulls=True).over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "dataset_ff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acd7aa-ea84-498b-be25-10ffdcdfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, count\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset_ff.count()\n",
    "\n",
    "# Count rows where 'Open' is available but any one of 'InterestRate', 'BondRate', or 'InflationRate' is missing (Null or NaN)\n",
    "missing_any_column_count = dataset_ff.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"BondRate\").isNull() | isnan(col(\"BondRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing any one of 'InterestRate', 'BondRate', or 'InflationRate': {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing any one rate: {percentage_missing_any_column:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0720-ee51-46c6-8f65-26a5d98f7a7f",
   "metadata": {},
   "source": [
    "Here's why the Window function is used:\n",
    "\n",
    "    Row-wise Calculations: PySpark's Window function allows you to specify a range of rows (a window) relative to the current row and perform calculations across these rows.\n",
    "\n",
    "    Emulating ffill: By ordering the rows in the window based on the 'date' column and then using the last function (which returns the last non-null value in the window), we effectively emulate the forward fill behavior. It picks the last non-null value encountered in the ordered window and applies it to the current row if the current value is null.\n",
    "\n",
    "    Scalability and Performance: PySpark is designed for big data processing. While Pandas' ffill is straightforward and efficient for in-memory data on a single machine, PySpark's window functions are designed to work efficiently on distributed datasets across multiple nodes in a cluster.\n",
    "\n",
    "    Lack of Direct ffill Equivalent: PySpark DataFrame API does not have a direct method like ffill in Pandas. Therefore, the combination of Window and last functions is a common approach to achieve similar functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca8e98-4285-4e30-9fb5-e02a0976a185",
   "metadata": {},
   "source": [
    "### Because of the Nature of the Fields (Interest Rate and Inflation Rate), is possible to use the MEAN to fill the Missing Day Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fc724-1340-4dda-b937-3b8ad88963a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Define the numeric columns\n",
    "numeric_cols = [\"InterestRate\", \"BondRate\", \"InflationRate\"]\n",
    "\n",
    "# Calculate the mean of non-null values for each column\n",
    "mean_values = dataset_ff.select([mean(c).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "\n",
    "# Fill null values with the mean\n",
    "for col in numeric_cols:\n",
    "    dataset_ff = dataset_ff.na.fill({col: mean_values[col]})\n",
    "\n",
    "# Show the result\n",
    "dataset_ff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620395c1-be42-4fef-b572-d878a14eb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, count\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset_ff.count()\n",
    "\n",
    "# Count rows where 'Open' is available but any one of 'InterestRate', 'BondRate', or 'InflationRate' is missing (Null or NaN)\n",
    "missing_any_column_count = dataset_ff.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"BondRate\").isNull() | isnan(col(\"BondRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing any one of 'InterestRate', 'BondRate', or 'InflationRate': {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing any one rate: {percentage_missing_any_column:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161b8d8-262e-4e01-8df9-2895f2c064a7",
   "metadata": {},
   "source": [
    "## KNN Approuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6f2cf-270c-424a-befa-0e0645596058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "dataset_pd = dataset.toPandas()\n",
    "\n",
    "# Replace non-numeric values to NaN\n",
    "numeric_cols = [\"InterestRate\", \"BondRate\", \"InflationRate\"]\n",
    "dataset_pd[numeric_cols] = dataset_pd[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "dataset_pd[numeric_cols] = knn_imputer.fit_transform(dataset_pd[numeric_cols])\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "dataset_knn = spark.createDataFrame(dataset_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d842e-51a4-463c-a05e-e766461d9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame in ascending order by the 'date' column\n",
    "dataset_knn = dataset_knn.orderBy(\"Date\")\n",
    "\n",
    "# Show the result\n",
    "dataset_knn.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e162a-beaf-458a-9cbd-0156b604b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, count\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = dataset_knn.count()\n",
    "\n",
    "# Count rows where 'Open' is available but any one of 'InterestRate', 'BondRate', or 'InflationRate' is missing (Null or NaN)\n",
    "missing_any_column_count = dataset_knn.filter(\n",
    "    col(\"Close\").isNotNull() &\n",
    "    (\n",
    "        col(\"InterestRate\").isNull() | isnan(col(\"InterestRate\")) |\n",
    "        col(\"BondRate\").isNull() | isnan(col(\"BondRate\")) |\n",
    "        col(\"InflationRate\").isNull() | isnan(col(\"InflationRate\"))\n",
    "    )\n",
    ").count()\n",
    "\n",
    "# Calculate the percentage of such instances\n",
    "percentage_missing_any_column = (missing_any_column_count / total_rows) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Rows with 'Open' but missing any one of 'InterestRate', 'BondRate', or 'InflationRate': {missing_any_column_count}\")\n",
    "print(f\"Percentage of Rows with missing any one rate: {percentage_missing_any_column:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c1cda-e37f-4123-b057-83d3ce84c12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44bc0a3b-c63b-41b5-a6d1-38089e2af0a4",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d97695-674a-4397-bf9b-3b7a96d855f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "dataset_knn_pd = dataset_knn.toPandas()\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(dataset_knn_pd.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac529a2f-4180-42e6-9de8-59ec877f2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "dataset_knn_pd = dataset_knn.toPandas()\n",
    "\n",
    "# Exclude non-numeric columns for correlation computation\n",
    "numeric_dataset = dataset_knn_pd.select_dtypes(include=[float, int])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = numeric_dataset.corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5f892-23da-4e1f-81a9-744cb2cc1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# List of key metrics to analyze\n",
    "metrics = ['Close', 'SMA', 'EMA', 'InterestRate', 'BondRate', 'InflationRate']\n",
    "\n",
    "# Plotting the distribution for each metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(dataset_knn_pd[metric], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {metric}')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7a384-c3b7-42aa-a8c1-a213ce23fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting time series for each metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='Date', y=metric, data=dataset_knn_pd)\n",
    "    plt.title(f'Time Series of {metric}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49c621-611c-419b-9fff-5adba99a9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "dataset_knn_pd = dataset_knn.toPandas()\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format and set as the index\n",
    "dataset_knn_pd['Date'] = pd.to_datetime(dataset_knn_pd['Date'])\n",
    "dataset_knn_pd.set_index('Date', inplace=True)\n",
    "\n",
    "# Filter for the most recent year\n",
    "max_date = dataset_knn_pd.index.max()\n",
    "one_year_ago = max_date - pd.DateOffset(years=1)\n",
    "recent_year_data = dataset_knn_pd[dataset_knn_pd.index >= one_year_ago]\n",
    "\n",
    "# Select the 'Close' column for analysis\n",
    "close_series_recent = recent_year_data['Close']\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "result = seasonal_decompose(close_series_recent, model='additive', period=365)\n",
    "\n",
    "# Plot the decomposed components\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ae028-4442-46dc-8b3e-f351dfc2eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "dataset_knn_pd = dataset_knn.toPandas()\n",
    "\n",
    "# Make sure the 'Date' column is in datetime format and set as the index\n",
    "dataset_knn_pd['Date'] = pd.to_datetime(dataset_knn_pd['Date'])\n",
    "dataset_knn_pd.set_index('Date', inplace=True)\n",
    "\n",
    "# Filter for the most recent year\n",
    "max_date = dataset_knn_pd.index.max()\n",
    "one_year_ago = max_date - pd.DateOffset(years=1)\n",
    "recent_year_data = dataset_knn_pd[dataset_knn_pd.index >= one_year_ago]\n",
    "\n",
    "# Select the 'Close' column\n",
    "close_series_recent = recent_year_data['Close']\n",
    "\n",
    "# Decompose to extract the trend component\n",
    "decomposition = seasonal_decompose(close_series_recent, model='additive', period=365)\n",
    "trend = decomposition.trend\n",
    "\n",
    "# Plotting the trend\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trend, label='Trend of Close Price')\n",
    "plt.title('Trend Component of Close Price - Last Year')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad5ceb-cf3a-48dc-be9b-0fc9a1ee204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the seasonal component\n",
    "seasonal_component = decomposition.seasonal\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seasonal_component, label='Seasonal Component of Close Price')\n",
    "plt.title('Seasonal Component of Close Price - Last Year')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Seasonal Variation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c9144-3edb-4631-86f8-58b1ec68b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the observed data\n",
    "observed_component = decomposition.observed\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(observed_component, label='Observed Close Price')\n",
    "plt.title('Observed Close Price - Last Year')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf0bbc-f1c5-4741-9cfd-741a46dd6b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de557a52-64a7-4bb9-a881-d2d189660a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create a trend plot for Inflation Rate\n",
    "def create_trend_plot(dataframe, title):\n",
    "    # Ensure 'Date' is the index and is in datetime format\n",
    "    dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "    dataframe.set_index('Date', inplace=True)\n",
    "\n",
    "    # Decompose to extract the trend component\n",
    "    result = seasonal_decompose(dataframe['InflationRate'], model='additive', period=365)\n",
    "    \n",
    "    # Plotting the trend component\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(result.trend)\n",
    "    plt.title(f'Trend of Inflation Rate - {title}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Inflation Rate')\n",
    "    plt.show()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame if not already done\n",
    "dataset_knn_pd = dataset_knn.toPandas()\n",
    "dataset_ff_pd = dataset_ff.toPandas()\n",
    "\n",
    "# Create trend plots\n",
    "create_trend_plot(dataset_knn_pd, \"KNN Imputed Data\")\n",
    "create_trend_plot(dataset_ff_pd, \"Forward Filled Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3893e-6ab3-4dff-8eab-5c055fb84dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seasonal_plot(dataframe, title):\n",
    "    # Check if 'Date' column exists, and if so, convert it to datetime and set as index\n",
    "    if 'Date' in dataframe.columns:\n",
    "        dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "        dataframe.set_index('Date', inplace=True)\n",
    "\n",
    "    # Decompose to extract the seasonal component\n",
    "    result = seasonal_decompose(dataframe['InflationRate'], model='additive', period=365)\n",
    "    \n",
    "    # Plotting the seasonal component\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(result.seasonal)\n",
    "    plt.title(f'Seasonal Component of Inflation Rate - {title}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Inflation Rate')\n",
    "    plt.show()\n",
    "\n",
    "# Create seasonal plots\n",
    "create_seasonal_plot(dataset_knn_pd, \"KNN Imputed Data\")\n",
    "create_seasonal_plot(dataset_ff_pd, \"Forward Filled Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec165-0412-458b-910e-f99405b6a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_observed_plot(dataframe, title):\n",
    "    # Check if 'Date' column exists, and if so, convert it to datetime and set as index\n",
    "    if 'Date' in dataframe.columns:\n",
    "        dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "        dataframe.set_index('Date', inplace=True)\n",
    "\n",
    "    # Decompose to extract the observed component\n",
    "    result = seasonal_decompose(dataframe['InflationRate'], model='additive', period=365)\n",
    "    \n",
    "    # Plotting the observed component\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(result.observed)\n",
    "    plt.title(f'Observed Inflation Rate - {title}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Inflation Rate')\n",
    "    plt.show()\n",
    "\n",
    "# Create observed plots\n",
    "create_observed_plot(dataset_knn_pd, \"KNN Imputed Data\")\n",
    "create_observed_plot(dataset_ff_pd, \"Forward Filled Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4473dd-7602-4232-9360-69011212654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trend_plot_interest(dataframe, title):\n",
    "    if 'Date' in dataframe.columns:\n",
    "        dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "        dataframe.set_index('Date', inplace=True)\n",
    "\n",
    "    result = seasonal_decompose(dataframe['InterestRate'], model='additive', period=365)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(result.trend)\n",
    "    plt.title(f'Trend of Interest Rate - {title}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Interest Rate')\n",
    "    plt.show()\n",
    "\n",
    "create_trend_plot_interest(dataset_knn_pd, \"KNN Imputed Data\")\n",
    "create_trend_plot_interest(dataset_ff_pd, \"Forward Filled Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b1b42-6309-4c86-823b-4933e27b04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seasonal_plot_interest(dataframe, title):\n",
    "    if 'Date' in dataframe.columns:\n",
    "        dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "        dataframe.set_index('Date', inplace=True)\n",
    "\n",
    "    result = seasonal_decompose(dataframe['InterestRate'], model='additive', period=365)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(result.seasonal)\n",
    "    plt.title(f'Seasonal Component of Interest Rate - {title}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Interest Rate')\n",
    "    plt.show()\n",
    "\n",
    "create_seasonal_plot_interest(dataset_knn_pd, \"KNN Imputed Data\")\n",
    "create_seasonal_plot_interest(dataset_ff_pd, \"Forward Filled Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cedb1-9997-4c5c-a2b3-da85a10d7e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_observed_plot_interest(dataframe, title):\n",
    "    if 'Date' in dataframe.columns:\n",
    "        dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "        dataframe.set_index('Date', inplace=True)\n",
    "\n",
    "    result = seasonal_decompose(dataframe['InterestRate'], model='additive', period=365)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(result.observed)\n",
    "    plt.title(f'Observed Interest Rate - {title}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Interest Rate')\n",
    "    plt.show()\n",
    "\n",
    "create_observed_plot_interest(dataset_knn_pd, \"KNN Imputed Data\")\n",
    "create_observed_plot_interest(dataset_ff_pd, \"Forward Filled Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e1c1b-3eaa-4f78-bc4a-eb27e3d9594b",
   "metadata": {},
   "source": [
    "# Model: ARIMA & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f8363-118e-48a6-b8a5-2b37879b5026",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db086307-bbd0-4035-8ad3-3eb01cf6f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Assuming you have a Spark session and your DataFrame 'dataset'\n",
    "# spark = SparkSession.builder.appName(\"YourAppName\").getOrCreate()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "dataset_pd = dataset.toPandas()\n",
    "\n",
    "# Convert 'Date' column to datetime and set it as index\n",
    "dataset_pd['Date'] = pd.to_datetime(dataset_pd['Date'])\n",
    "\n",
    "# Ensure the dataset is sorted and has no duplicates\n",
    "dataset_pd = dataset_pd.sort_values('Date').drop_duplicates(subset='Date')\n",
    "\n",
    "# Resample to daily frequency if necessary, filling any missing values\n",
    "dataset_pd.set_index('Date', inplace=True)\n",
    "dataset_pd = dataset_pd.asfreq('D', method='pad')\n",
    "\n",
    "# Select the series for modeling, e.g., 'Close' price\n",
    "series = dataset_pd['Close']\n",
    "\n",
    "# Plot ACF and PACF for parameter identification\n",
    "plot_acf(series)\n",
    "plot_pacf(series)\n",
    "\n",
    "# Define and fit the ARIMA model (revised parameters based on ACF and PACF analysis)\n",
    "model = ARIMA(series, order=(1,1,1))  # Change order based on your ACF and PACF analysis\n",
    "results = model.fit()\n",
    "\n",
    "# Summarize model results\n",
    "print(results.summary())\n",
    "\n",
    "# Make predictions\n",
    "predictions = results.forecast(steps=5)\n",
    "\n",
    "# Evaluate the model\n",
    "# [Your code for evaluation, such as comparing predictions with actual values or calculating error metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c757f-4095-4190-8def-fab863e0cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import itertools\n",
    "\n",
    "# Assuming dataset_pd is your Pandas DataFrame\n",
    "\n",
    "# Check and adjust if 'Date' is already the index or named differently\n",
    "if 'Date' not in dataset_pd.columns:\n",
    "    # If 'Date' is not a column, it might already be the index or named differently\n",
    "    # If it's named differently, replace 'Date' with the correct column name\n",
    "    dataset_pd.index = pd.to_datetime(dataset_pd.index)\n",
    "else:\n",
    "    # Convert 'Date' column to datetime and set it as index\n",
    "    dataset_pd['Date'] = pd.to_datetime(dataset_pd['Date'])\n",
    "    dataset_pd.set_index('Date', inplace=True)\n",
    "\n",
    "# Remove duplicates if they exist\n",
    "dataset_pd = dataset_pd[~dataset_pd.index.duplicated(keep='first')]\n",
    "\n",
    "# Resample to daily frequency if necessary, filling any missing values\n",
    "dataset_pd = dataset_pd.asfreq('D', method='pad')\n",
    "series = dataset_pd['Close']\n",
    "\n",
    "# Define the p, d, and q parameters to take values between 0 and 2\n",
    "p = d = q = range(0, 3)\n",
    "\n",
    "# Generate all different combinations of p, d, and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Iterate over all combinations and fit ARIMA models\n",
    "model_results = []\n",
    "for param in pdq:\n",
    "    try:\n",
    "        model = ARIMA(series, order=param)\n",
    "        results = model.fit()\n",
    "        model_results.append({\n",
    "            'Order': param,\n",
    "            'AIC': results.aic,\n",
    "            'BIC': results.bic\n",
    "        })\n",
    "        print(f'ARIMA{param} - AIC:{results.aic} - BIC:{results.bic}')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Sort by AIC and BIC\n",
    "sorted_results_aic = results_df.sort_values(by='AIC')\n",
    "sorted_results_bic = results_df.sort_values(by='BIC')\n",
    "\n",
    "# Display the top models based on AIC and BIC\n",
    "print(\"\\nTop models based on AIC:\")\n",
    "print(sorted_results_aic.head())\n",
    "\n",
    "print(\"\\nTop models based on BIC:\")\n",
    "print(sorted_results_bic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ff30b-b152-441f-acf0-d0a975c6ff28",
   "metadata": {},
   "source": [
    "Interpretation of AIC and BIC\n",
    "\n",
    "    AIC (Akaike Information Criterion):\n",
    "        AIC is a measure of the relative quality of a statistical model for a given set of data. A lower AIC value suggests a better model.\n",
    "        It balances the complexity of the model (number of parameters) against how well the model fits the data.\n",
    "\n",
    "    BIC (Bayesian Information Criterion):\n",
    "        Similar to AIC, BIC provides a way to compare models. It penalizes more complex models more than AIC does.\n",
    "        A lower BIC value indicates a better model and is especially useful for models with a larger number of observations.\n",
    "\n",
    "Analysis of Your Results\n",
    "\n",
    "    Top Models by AIC:\n",
    "        The models with the lowest AIC are ARIMA(1, 1, 0), ARIMA(1, 1, 1), ARIMA(0, 1, 1), ARIMA(2, 1, 0), and ARIMA(0, 1, 2).\n",
    "        These models are preferable in terms of balancing model complexity and fit.\n",
    "\n",
    "    Top Models by BIC:\n",
    "        The models with the lowest BIC are ARIMA(0, 1, 0), ARIMA(1, 1, 0), ARIMA(0, 1, 1), ARIMA(0, 2, 1), and ARIMA(1, 1, 1).\n",
    "        These models are more parsimonious (simpler) and are generally preferred when dealing with larger datasets or when overfitting is a concern.\n",
    "\n",
    "Recommendations and Next Steps\n",
    "\n",
    "    Model Selection:\n",
    "        ARIMA(1, 1, 0) and ARIMA(1, 1, 1) appear in both top AIC and BIC lists, making them strong candidates. These models are likely to provide a good balance between complexity and fit.\n",
    "\n",
    "        In summary, the results indicate that simpler models (like ARIMA(1, 1, 0)) are performing well compared to more complex ones. This is a good indication that your time series might not require highly complex ARIMA configurations to achieve effective modeling and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad2f26-63d0-4c7f-a5e5-60b47ea1c8b2",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb001ce8-4f4a-40f6-bd87-e6d257fdbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "dataset_knn_pd = dataset_knn.toPandas()\n",
    "\n",
    "# Selecting the relevant columns for input features and the target variable 'Close'\n",
    "feature_columns = ['Close', 'SMA', 'EMA', 'InterestRate', 'BondRate', 'InflationRate']\n",
    "target_column = 'Close'\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset_knn_pd[feature_columns])\n",
    "\n",
    "# Function to create a dataset with lagged features\n",
    "def create_dataset(data, look_back=1):\n",
    "    X, Y = [], []\n",
    "    target_idx = dataset_knn_pd.columns.get_loc(target_column)\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        a = data[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + look_back, target_idx])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Prepare the dataset with a specified look back period\n",
    "look_back = 1\n",
    "X, Y = create_dataset(scaled_data, look_back)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.67)\n",
    "trainX, trainY = X[:train_size], Y[:train_size]\n",
    "testX, testY = X[train_size:], Y[train_size:]\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back, len(feature_columns)))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back, len(feature_columns)))\n",
    "\n",
    "# Build and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, len(feature_columns))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# Invert predictions\n",
    "trainPredict = scaler.inverse_transform(np.column_stack((trainPredict, np.zeros((trainPredict.shape[0], len(feature_columns)-1)))))\n",
    "testPredict = scaler.inverse_transform(np.column_stack((testPredict, np.zeros((testPredict.shape[0], len(feature_columns)-1)))))\n",
    "trainY_original = scaler.inverse_transform(np.column_stack((trainY.reshape(-1, 1), np.zeros((trainY.shape[0], len(feature_columns)-1)))))\n",
    "testY_original = scaler.inverse_transform(np.column_stack((testY.reshape(-1, 1), np.zeros((testY.shape[0], len(feature_columns)-1)))))\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(trainY_original[:, 0], trainPredict[:, 0]))\n",
    "test_rmse = np.sqrt(mean_squared_error(testY_original[:, 0], testPredict[:, 0]))\n",
    "test_mape = mean_absolute_percentage_error(testY_original[:, 0], testPredict[:, 0])\n",
    "test_r2 = r2_score(testY_original[:, 0], testPredict[:, 0])\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Train RMSE:', train_rmse)\n",
    "print('Test RMSE:', test_rmse)\n",
    "print('Test MAPE:', test_mape)\n",
    "print('Test R²:', test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a2a2a-cf9e-44fd-83ee-8ca01c3c5cb7",
   "metadata": {},
   "source": [
    "Step 1: Building the Basic LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248826ac-db25-4862-809c-9e89f55d1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def build_lstm_model(input_shape, neurons=50, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))  # Single output neuron for prediction\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74acf46-2cc7-4e15-a176-dd8e107a257e",
   "metadata": {},
   "source": [
    "Step 2: Data Preparation for LSTM Model\n",
    "Now, let's prepare the data for training the LSTM model. This involves normalizing the data and creating lagged features as input for the LSTM. We'll use the 'Close' price and additional indicators from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adfd8e-fdef-47e1-9290-62a68a53964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "feature_columns = ['Close', 'SMA', 'EMA', 'InterestRate', 'BondRate', 'InflationRate'] \n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset_knn_pd[feature_columns])\n",
    "\n",
    "# Function to create a dataset with lagged features\n",
    "def create_dataset(data, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back), :])\n",
    "        Y.append(data[i + look_back, 0])  #  'Close' is the first column\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Create the lagged dataset\n",
    "look_back = 1  # This can be adjusted\n",
    "X, Y = create_dataset(scaled_data, look_back)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_size = int(len(X) * 0.67)\n",
    "test_size = len(X) - train_size\n",
    "trainX, trainY = X[:train_size], Y[:train_size]\n",
    "testX, testY = X[train_size:], Y[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045761b6-0720-4d25-bfd1-0b431f0e2ebe",
   "metadata": {},
   "source": [
    "Step 3: Basic Model Training\n",
    "\n",
    "With the  data prepared, the next step is to train the LSTM model with a basic set of hyperparameters. We'll start with a single LSTM layer and a standard configuration, then build from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430a824-5d71-4ab7-9b09-772a7ae79297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Build the LSTM model\n",
    "input_shape = (look_back, len(feature_columns))\n",
    "model = build_lstm_model(input_shape, neurons=50, dropout_rate=0.2)  # Basic configuration\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=32, validation_split=0.2, verbose=0)  # Basic training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76dbfe-3042-47db-85d7-ff3f75e7c25e",
   "metadata": {},
   "source": [
    "Step 4: Model Evaluation on Test Data\n",
    "\n",
    "Great, The model has been trained! Now, let's evaluate its performance on the test data. We'll make predictions and then calculate performance metrics like RMSE (Root Mean Squared Error) to assess how well the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02c526-7575-42c8-9bff-a412e1d874cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Calculate MAPE\n",
    "test_mape = mean_absolute_percentage_error(testY_original[:, 0], testPredict[:, 0])\n",
    "\n",
    "# Calculate R² Score\n",
    "test_r2 = r2_score(testY_original[:, 0], testPredict[:, 0])\n",
    "\n",
    "print('Test RMSE:', test_rmse)\n",
    "print('Test MAPE:', test_mape)\n",
    "print('Test R²:', test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f94ce-3dff-4bd9-a494-3360bcc09d4d",
   "metadata": {},
   "source": [
    "Step 5: Hyperparameter Tuning\n",
    "\n",
    "Now that the evaluated the basic model, the next step is to perform hyperparameter tuning. This involves systematically experimenting with different model configurations to improve performance. We'll focus on tuning key hyperparameters like the number of neurons and learning rate.\n",
    "\n",
    "Given the iterative nature of this process, it's efficient to create a function that handles model creation, training, and evaluation for different sets of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061190cb-e5d8-4043-962d-e5f638c2078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def hyperparameter_tuning(trainX, trainY, testX, testY, neurons_list, learning_rates, batch_sizes, optimizers):\n",
    "    results = []\n",
    "    \n",
    "    for neurons in neurons_list:\n",
    "        for optimizer in optimizers:\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Build the model\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(neurons, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    model.add(Dense(1))\n",
    "\n",
    "                    # Compile the model\n",
    "                    if optimizer == 'adam':\n",
    "                        model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "                    elif optimizer == 'nadam':\n",
    "                        model.compile(optimizer=Nadam(learning_rate=lr), loss='mean_squared_error')\n",
    "                    elif optimizer == 'adagrad':\n",
    "                        model.compile(optimizer=Adagrad(learning_rate=lr), loss='mean_squared_error')\n",
    "\n",
    "                    # Train the model\n",
    "                    model.fit(trainX, trainY, epochs=100, batch_size=batch_size, verbose=0)\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    testPredict = model.predict(testX)\n",
    "                    test_rmse = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "                    test_mape = mean_absolute_percentage_error(testY, testPredict)\n",
    "                    test_r2 = r2_score(testY, testPredict)\n",
    "\n",
    "                    # Record the results\n",
    "                    results.append({\n",
    "                        'Neurons': neurons,\n",
    "                        'Optimizer': optimizer,\n",
    "                        'Learning Rate': lr,\n",
    "                        'Batch Size': batch_size,\n",
    "                        'Test RMSE': test_rmse,\n",
    "                        'Test MAPE': test_mape,\n",
    "                        'Test R²': test_r2\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "neurons_list = [10, 30, 50, 100, 150, 200]\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "optimizers = ['adam', 'nadam', 'adagrad']\n",
    "\n",
    "# Example usage with your train and test datasets\n",
    "results = hyperparameter_tuning(trainX, trainY, testX, testY, neurons_list, learning_rates, batch_sizes, optimizers)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48602cc-3979-4459-961a-8a359ac255d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5cc6f-288f-4739-9806-80933da9b668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5f8ad-9cc3-4812-a0f8-1ec25c56a966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d42e8-6dd6-4c41-97f0-e75b507a05ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8565c5a-a82b-4d86-85f0-ab37ec422cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fe70b-3ad9-4477-91b9-70e7a729a738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
